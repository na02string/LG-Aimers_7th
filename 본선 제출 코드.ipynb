{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06d7df36",
   "metadata": {},
   "source": [
    "# 딥\n",
    "\n",
    "===== OS / Python 환경 =====\n",
    "\n",
    "OS: Linux 6.8.0-1032-oracle #33~22.04.1-Ubuntu SMP Thu Aug 14 01:46:51 UTC 2025\n",
    "\n",
    "Machine: x86_64\n",
    "\n",
    "Processor: x86_64\n",
    "\n",
    "Python: 3.12.2 | packaged by conda-forge | (main, Feb 16 2024, 20:50:58) [GCC 12.3.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c3c6d65-4426-40d4-98e1-8e2cb3e4978b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "====================================\n",
      "       Training LSTM Model\n",
      "====================================\n",
      "[01/25] train_loss=0.7457  valid_SCORE=0.374071\n",
      "[02/25] train_loss=0.5709  valid_SCORE=0.403262\n",
      "[03/25] train_loss=0.5331  valid_SCORE=0.415332\n",
      "[04/25] train_loss=0.5053  valid_SCORE=0.422289\n",
      "[05/25] train_loss=0.4837  valid_SCORE=0.428581\n",
      "[06/25] train_loss=0.4671  valid_SCORE=0.453441\n",
      "[07/25] train_loss=0.4526  valid_SCORE=0.435822\n",
      "[08/25] train_loss=0.4388  valid_SCORE=0.442384\n",
      "[09/25] train_loss=0.4262  valid_SCORE=0.450633\n",
      "[10/25] train_loss=0.4132  valid_SCORE=0.462772\n",
      "[11/25] train_loss=0.4018  valid_SCORE=0.472433\n",
      "[12/25] train_loss=0.3910  valid_SCORE=0.446792\n",
      "[13/25] train_loss=0.3812  valid_SCORE=0.458323\n",
      "[14/25] train_loss=0.3703  valid_SCORE=0.457380\n",
      "[15/25] train_loss=0.3596  valid_SCORE=0.449387\n",
      "[16/25] train_loss=0.3507  valid_SCORE=0.475510\n",
      "[17/25] train_loss=0.3420  valid_SCORE=0.458952\n",
      "[18/25] train_loss=0.3332  valid_SCORE=0.465481\n",
      "[19/25] train_loss=0.3253  valid_SCORE=0.482427\n",
      "[20/25] train_loss=0.3175  valid_SCORE=0.477867\n",
      "[21/25] train_loss=0.3094  valid_SCORE=0.494188\n",
      "[22/25] train_loss=0.3019  valid_SCORE=0.488821\n",
      "[23/25] train_loss=0.2941  valid_SCORE=0.487246\n",
      "[24/25] train_loss=0.2876  valid_SCORE=0.484141\n",
      "[25/25] train_loss=0.2809  valid_SCORE=0.470745\n",
      "=> Best valid SCORE: 0.494188  (best_lstm.pt)\n",
      "====================================\n",
      "       Training N-HiTS Model\n",
      "====================================\n",
      "[01/25] train_loss=0.6772  valid_SCORE=0.354803\n",
      "[02/25] train_loss=0.6018  valid_SCORE=0.412908\n",
      "[03/25] train_loss=0.5874  valid_SCORE=0.352353\n",
      "[04/25] train_loss=0.5803  valid_SCORE=0.376695\n",
      "[05/25] train_loss=0.5737  valid_SCORE=0.361481\n",
      "[06/25] train_loss=0.5674  valid_SCORE=0.349686\n",
      "[07/25] train_loss=0.5634  valid_SCORE=0.347659\n",
      "Early stopping.\n",
      "=> Best valid SCORE: 0.412908  (best_nhits.pt)\n",
      "[Temp Tuning] LSTM best τ=1.0 → valid_SCORE=0.494188\n",
      "[Temp Tuning] N-HiTS best τ=1.5 → valid_SCORE=0.424604\n",
      "[Validation SCORE] LSTM(τ*)=0.494188  N-HiTS(τ*)=0.424604  Global Blend a=0.80: 0.501447\n",
      "Tuned groups (alpha): 8 / 8\n",
      "======== Validation Summary ========\n",
      "LSTM (best τ=1.0): 0.494188\n",
      "N-HiTS (best τ=1.5): 0.424604\n",
      "Global Blend (α=0.80): 0.501447\n",
      "Group Blend (tuned α): 0.509248\n",
      "[Saved] ../data/submission_groupblend_long.csv  shape=(11690, 3)\n",
      "[Saved] ../data/submission.csv (WIDE aligned) shape=(70, 168)\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# Final Round — Robust, GPU-ready, Error-proof\n",
    "# LSTM + Lightweight N-HiTS (two-head) with full feature pipeline\n",
    "# - Paths: use exactly the competitionㅁ layout you provided\n",
    "# - New metric: (A + B + C + D) / 4 maximized\n",
    "# - No KeyError on '영업장명_메뉴명' (header/key sanitization)\n",
    "# - No NaN/inf in log1p\n",
    "# - No interop thread errors (no set_num_interop_threads)\n",
    "# =========================================================\n",
    "\n",
    "# ----------------------------- Setup -----------------------------\n",
    "import os, re, glob, random, json\n",
    "SEED = 42\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"MKL_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"NUMEXPR_NUM_THREADS\", \"1\")\n",
    "os.environ.setdefault(\"CUBLAS_WORKSPACE_CONFIG\", \":16:8\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Holidays (fallback if not installed)\n",
    "try:\n",
    "    import holidays\n",
    "    HAVE_HOLIDAYS = True\n",
    "except Exception:\n",
    "    HAVE_HOLIDAYS = False\n",
    "\n",
    "# RNG/Determinism\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "torch.use_deterministic_algorithms(True, warn_only=False)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "torch.backends.cudnn.allow_tf32 = False\n",
    "torch.set_float32_matmul_precision(\"high\")  # ok on CPU/GPU\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# -------------------------- Paths & HPs --------------------------\n",
    "LOOKBACK, HORIZON = 28, 7\n",
    "BATCH, EPOCHS, LR = 256, 25, 1e-3\n",
    "PATIENCE = 5\n",
    "\n",
    "# ---- Use the exact paths you provided ----\n",
    "DATA_PATH = \"../data\"\n",
    "PRICE_PATH = DATA_PATH+\"/train/price.csv\"\n",
    "ROOM_TYPE_PATH = DATA_PATH+\"/train/room_type.csv\"\n",
    "TRAIN_PATH = DATA_PATH+\"/train/train.csv\"\n",
    "TRAIN_group_PATH   = DATA_PATH+\"/train/meta/TRAIN_group.csv\"\n",
    "TRAIN_hwadam_PATH  = DATA_PATH+\"/train/meta/TRAIN_hwadam.csv\"\n",
    "TRAIN_room_PATH    = DATA_PATH+\"/train/meta/TRAIN_room.csv\"\n",
    "TRAIN_ski_PATH     = DATA_PATH+\"/train/meta/TRAIN_ski.csv\"\n",
    "TRAIN_weather_PATH = DATA_PATH+\"/train/meta/TRAIN_weather.csv\"\n",
    "TEST_DIR      = DATA_PATH+\"/test\"     # TEST_00.csv ~ TEST_09.csv\n",
    "TEST_meta_DIR = DATA_PATH+\"/test/meta\"\n",
    "SAMPLE_PATH   = DATA_PATH+\"/sample_submission.csv\"\n",
    "SUBMIT_PATH   = DATA_PATH+\"/submission.csv\"\n",
    "\n",
    "# Blending/temperature\n",
    "GROUPING = 'store'        # 'store' | 'category' | 'store_category'\n",
    "MIN_ITEMS_PER_GROUP = 3\n",
    "GLOBAL_ALPHA = 0.80\n",
    "ALPHAS_GRID = np.arange(0.40, 1.01, 0.01)\n",
    "TEMP_GRID   = [0.5, 0.75, 1.0, 1.25, 1.5]\n",
    "\n",
    "# ---------------------- Safe torch.load helper -------------------\n",
    "def _load_state_dict_safe(path, map_location=None):\n",
    "    # Avoids future pickle warning (PyTorch 2.5+). Try weights_only.\n",
    "    try:\n",
    "        return torch.load(path, map_location=map_location, weights_only=True)\n",
    "    except TypeError:\n",
    "        return torch.load(path, map_location=map_location)\n",
    "\n",
    "# ---------------------- Column/Key Sanitizers --------------------\n",
    "CANON_KEY = \"영업장명_메뉴명\"\n",
    "\n",
    "def sanitize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    clean = []\n",
    "    for c in df.columns:\n",
    "        s = str(c)\n",
    "        s = s.replace(\"\\ufeff\",\"\").replace(\"\\u200b\",\"\").replace(\"\\xa0\",\" \")\n",
    "        s = s.strip()\n",
    "        clean.append(s)\n",
    "    df.columns = clean\n",
    "    return df\n",
    "\n",
    "def sanitize_key_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if CANON_KEY in df.columns:\n",
    "        df[CANON_KEY] = (df[CANON_KEY].astype(str)\n",
    "                         .str.replace(\"\\ufeff|\\u200b\", \"\", regex=True)\n",
    "                         .str.replace(\"\\xa0\", \" \", regex=False)\n",
    "                         .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "                         .str.strip())\n",
    "    return df\n",
    "\n",
    "def normalize_key_column(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = sanitize_columns(df)\n",
    "    df = df.copy()\n",
    "    if CANON_KEY in df.columns:\n",
    "        return sanitize_key_values(df)\n",
    "\n",
    "    cand = [c for c in df.columns\n",
    "            if ('메뉴' in c) and (('영업장' in c) or ('매장' in c) or ('업장' in c) or ('지점' in c))]\n",
    "    if len(cand) == 1:\n",
    "        df = df.rename(columns={cand[0]: CANON_KEY})\n",
    "        return sanitize_key_values(df)\n",
    "\n",
    "    def _mk(sm_col, mn_col):\n",
    "        return (df[sm_col].astype(str).str.strip() + \"_\" + df[mn_col].astype(str).str.strip())\n",
    "\n",
    "    for s_col in ['영업장명','매장명','업장명','지점명']:\n",
    "        if s_col in df.columns and '메뉴명' in df.columns:\n",
    "            df[CANON_KEY] = _mk(s_col, '메뉴명')\n",
    "            return sanitize_key_values(df)\n",
    "\n",
    "    raise KeyError(f\"'{CANON_KEY}' 컬럼을 찾거나 만들 수 없습니다. 현재 컬럼: {list(df.columns)}\")\n",
    "\n",
    "# ---------------------- IO helpers & features --------------------\n",
    "def read_csv_safe(path, **kwargs):\n",
    "    for enc in [None, \"utf-8-sig\", \"cp949\", \"euc-kr\"]:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, **kwargs)\n",
    "        except Exception:\n",
    "            pass\n",
    "    raise RuntimeError(f\"파일 로드 실패: {path}\")\n",
    "\n",
    "def preprocess_price(price_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    key_col, price_col = CANON_KEY, \"평균판매금액\"\n",
    "    df = normalize_key_column(price_df)\n",
    "    df[price_col] = pd.to_numeric(df[price_col], errors=\"coerce\")\n",
    "    miss0 = df[price_col].isna() | (df[price_col] == 0)\n",
    "\n",
    "    # 담하 (정식) 보정\n",
    "    is_damha   = df[key_col].str.contains(\"담하\", na=False)\n",
    "    is_jungsik = df[key_col].str.contains(r\"\\(정식\\)\", na=False)\n",
    "    damha_mean = df.loc[is_damha & is_jungsik & ~miss0, price_col].mean()\n",
    "    if np.isnan(damha_mean):\n",
    "        damha_mean = df.loc[is_damha & ~miss0, price_col].mean()\n",
    "    df.loc[is_damha & is_jungsik & miss0, price_col] = damha_mean\n",
    "\n",
    "    # 고정값\n",
    "    fixed = {\"미라시아_브런치 2인 패키지\": 81000, \"미라시아_브런치 4인 패키지\": 162000}\n",
    "    for k, v in fixed.items():\n",
    "        df.loc[(df[key_col] == k) & miss0, price_col] = v\n",
    "\n",
    "    # 오픈푸드 → 0\n",
    "    openfood = df[key_col].str.contains(\"Open Food|오픈푸드\", na=False)\n",
    "    df.loc[openfood & df[price_col].isna(), price_col] = 0\n",
    "    return df[[key_col, price_col]].drop_duplicates(subset=[key_col], keep=\"last\")\n",
    "\n",
    "# 객실 타입 메타\n",
    "room_type_raw = read_csv_safe(ROOM_TYPE_PATH)\n",
    "room_type_raw = sanitize_columns(room_type_raw)\n",
    "room_type = room_type_raw.rename(columns={\"객실타입\": \"room_code\"})\n",
    "room_type_meta = room_type[['room_code', '객실타입명']].copy()\n",
    "\n",
    "def build_room_ratio(room_csv_path: str, room_type_meta: pd.DataFrame) -> pd.DataFrame:\n",
    "    room_wide = read_csv_safe(room_csv_path)\n",
    "    room_wide = sanitize_columns(room_wide)\n",
    "    room_wide[\"영업일자\"] = pd.to_datetime(room_wide[\"영업일자\"])\n",
    "    room_long = room_wide.melt(id_vars=[\"영업일자\"], var_name=\"room_code\", value_name=\"room_count\")\n",
    "    room_long[\"room_count\"] = pd.to_numeric(room_long[\"room_count\"], errors=\"coerce\").fillna(0)\n",
    "    room_long = room_long.merge(room_type_meta, on=\"room_code\", how=\"left\")\n",
    "    room_long[\"취사여부\"] = room_long[\"객실타입명\"].fillna(\"\").str.contains(\"취사\").astype(int)\n",
    "    room_long[\"cook_cnt\"] = room_long[\"취사여부\"] * room_long[\"room_count\"]\n",
    "\n",
    "    daily = room_long.groupby(\"영업일자\", as_index=False).agg(\n",
    "        total_rooms   = (\"room_count\",\"sum\"),\n",
    "        cooking_rooms = (\"cook_cnt\",\"sum\")\n",
    "    )\n",
    "    daily[\"cooking_ratio\"] = np.where(\n",
    "        daily[\"total_rooms\"] > 0,\n",
    "        daily[\"cooking_rooms\"] / daily[\"total_rooms\"],\n",
    "        np.nan\n",
    "    )\n",
    "    return daily[[\"영업일자\", \"cooking_ratio\"]]\n",
    "\n",
    "def attach_ratio(df: pd.DataFrame, ratio_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[\"영업일자\"] = pd.to_datetime(out[\"영업일자\"])\n",
    "    ratio_df = ratio_df.copy()\n",
    "    ratio_df[\"영업일자\"] = pd.to_datetime(ratio_df[\"영업일자\"])\n",
    "    return out.merge(ratio_df, on=\"영업일자\", how=\"left\")\n",
    "\n",
    "def _coerce_ski_colnames(sdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    sdf = sanitize_columns(sdf)\n",
    "    cols = {c: c.replace(\" \", \"\") for c in sdf.columns}\n",
    "    sdf = sdf.rename(columns=cols)\n",
    "    if \"1일내장객\" not in sdf.columns:\n",
    "        raise KeyError(\"스키 데이터에 '1일내장객' 컬럼이 없습니다.\")\n",
    "    return sdf\n",
    "\n",
    "def attach_daily_ski(df: pd.DataFrame, sdf: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[\"영업일자\"]  = pd.to_datetime(out[\"영업일자\"], errors=\"coerce\")\n",
    "    sdf = _coerce_ski_colnames(sdf)\n",
    "    sdf[\"영업일자\"] = pd.to_datetime(sdf[\"영업일자\"], errors=\"coerce\")\n",
    "    sdf[\"1일내장객\"] = pd.to_numeric(sdf[\"1일내장객\"], errors=\"coerce\")\n",
    "    daily = sdf.groupby(\"영업일자\", as_index=False)[\"1일내장객\"].max()\n",
    "    return out.merge(daily, on=\"영업일자\", how=\"left\").fillna({\"1일내장객\":0})\n",
    "\n",
    "def add_time_features(df, date_col='영업일자'):\n",
    "    out = df.copy()\n",
    "    out[date_col] = pd.to_datetime(out[date_col])\n",
    "    dt = out[date_col].dt\n",
    "    month = dt.month\n",
    "    out['일']   = dt.day\n",
    "    out['요일'] = dt.dayofweek\n",
    "    out['주차'] = dt.isocalendar().week.astype('int32')\n",
    "    out['계절'] = month.map(lambda m: 0 if m in [12,1,2] else 1 if m in [3,4,5] else 2 if m in [6,7,8] else 3)\n",
    "    # Holidays (safe fallback to empty)\n",
    "    if HAVE_HOLIDAYS:\n",
    "        years = sorted(out[date_col].dt.year.unique())\n",
    "        kr_holidays = set(holidays.KR(years=years).keys())\n",
    "    else:\n",
    "        kr_holidays = set()\n",
    "    out['공휴일여부'] = out[date_col].dt.date.isin(kr_holidays).astype(int)\n",
    "    out['월_sin']   = np.sin(2*np.pi*month/12)\n",
    "    out['월_cos']   = np.cos(2*np.pi*month/12)\n",
    "    out['요일_sin'] = np.sin(2*np.pi*out['요일']/7)\n",
    "    out['요일_cos'] = np.cos(2*np.pi*out['요일']/7)\n",
    "    return out\n",
    "\n",
    "def add_calendar_context_features(df: pd.DataFrame, date_col='영업일자') -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[date_col] = pd.to_datetime(out[date_col])\n",
    "    d = out[date_col].dt\n",
    "    if 'is_weekend' not in out.columns:\n",
    "        out['is_weekend'] = (d.dayofweek >= 5).astype(int)\n",
    "    if HAVE_HOLIDAYS:\n",
    "        years = sorted(out[date_col].dt.year.unique())\n",
    "        kr_hdays = set(holidays.KR(years=years).keys())\n",
    "    else:\n",
    "        kr_hdays = set()\n",
    "    prev_day = (out[date_col] - pd.Timedelta(days=1)).dt.date\n",
    "    next_day = (out[date_col] + pd.Timedelta(days=1)).dt.date\n",
    "    out['전일_공휴일'] = prev_day.map(lambda x: int(x in kr_hdays))\n",
    "    out['익일_공휴일'] = next_day.map(lambda x: int(x in kr_hdays))\n",
    "    if '공휴일여부' in out.columns:\n",
    "        out['휴일전날']   = ((out['공휴일여부'] == 0) & (out['익일_공휴일'] == 1)).astype(int)\n",
    "        out['휴일다음날'] = ((out['공휴일여부'] == 0) & (out['전일_공휴일'] == 1)).astype(int)\n",
    "        out['연휴중여부'] = (((out['전일_공휴일'] == 1) | (out['익일_공휴일'] == 1)) & (out['공휴일여부'] == 1)).astype(int)\n",
    "    else:\n",
    "        out['휴일전날']   = (out['익일_공휴일'] == 1).astype(int)\n",
    "        out['휴일다음날'] = (out['전일_공휴일'] == 1).astype(int)\n",
    "        out['연휴중여부'] = ((out['전일_공휴일'] == 1) | (out['익일_공휴일'] == 1)).astype(int)\n",
    "    return out\n",
    "\n",
    "def add_filtered_ts_features(\n",
    "    df: pd.DataFrame,\n",
    "    group_cols=(CANON_KEY,),\n",
    "    date_col='영업일자',\n",
    "    target_col='매출수량',\n",
    "    lags=(7, 14),\n",
    "    roll_mean_windows=(1, 3, 7, 14, 28),\n",
    "    roll_std_windows=(3, 7, 14, 28),\n",
    "):\n",
    "    out = df.sort_values(list(group_cols)+[date_col]).copy()\n",
    "    g = out.groupby(list(group_cols))[target_col]\n",
    "    prev = g.shift(1)\n",
    "    keys = [out[c] for c in group_cols]\n",
    "\n",
    "    prev_exp_mean = (\n",
    "        prev.groupby(keys)\n",
    "            .expanding(min_periods=1).mean()\n",
    "            .reset_index(level=list(range(len(group_cols))), drop=True)\n",
    "    ).fillna(0.0)\n",
    "\n",
    "    for k in lags:\n",
    "        out[f'lag_{k}'] = g.shift(k).fillna(prev_exp_mean)\n",
    "\n",
    "    for w in roll_mean_windows:\n",
    "        out[f'roll_mean_{w}'] = (\n",
    "            prev.groupby(keys).rolling(w, min_periods=1).mean()\n",
    "                .reset_index(level=list(range(len(group_cols))), drop=True)\n",
    "        ).fillna(0.0)\n",
    "\n",
    "    for w in roll_std_windows:\n",
    "        out[f'roll_std_{w}'] = (\n",
    "            prev.groupby(keys).rolling(w, min_periods=2).std()\n",
    "                .reset_index(level=list(range(len(group_cols))), drop=True)\n",
    "        ).fillna(0.0)\n",
    "\n",
    "    return out\n",
    "\n",
    "def finalize_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    key = CANON_KEY\n",
    "    out = add_time_features(df, date_col='영업일자')\n",
    "    out = add_calendar_context_features(out, date_col='영업일자')\n",
    "    out = out.sort_values([key, '영업일자']).copy()\n",
    "\n",
    "    # wd_mean_local_28 (apply 없이 — 키 증발 방지)\n",
    "    s = pd.to_numeric(out['매출수량'], errors='coerce').fillna(0.0)\n",
    "    same_wd_mean = (\n",
    "        s.groupby(out[key]).shift(7)\n",
    "         .groupby(out[key]).rolling(4, min_periods=1).mean()\n",
    "         .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    all_wd_mean = (\n",
    "        s.groupby(out[key]).shift(1)\n",
    "         .groupby(out[key]).rolling(7, min_periods=1).mean()\n",
    "         .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    out['wd_mean_local_28'] = same_wd_mean.where(~same_wd_mean.isna(), all_wd_mean).fillna(0.0).astype(float)\n",
    "\n",
    "    out = add_filtered_ts_features(out, group_cols=(key,),\n",
    "                                   date_col='영업일자', target_col='매출수량')\n",
    "    return out\n",
    "\n",
    "def add_venue_group_features(df: pd.DataFrame,\n",
    "                             col=CANON_KEY,\n",
    "                             keep_venue_name=True) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    def _extract_venue(x: str) -> str:\n",
    "        return str(x).split(\"_\",1)[0] if pd.notna(x) else \"\"\n",
    "    venue = out[col].astype(str).map(_extract_venue)\n",
    "    if keep_venue_name:\n",
    "        out['영업장명'] = venue\n",
    "\n",
    "    ski_venues     = {\"카페테리아\",\"포레스트릿\"}\n",
    "    hwadam_venues  = {\"화담숲주막\",\"화담숲카페\"}\n",
    "    lodging_venues = {\"느티나무 셀프BBQ\",\"연회장\",\"담하\",\"미라시아\",\"라그로타\"}\n",
    "    def _map_group(v):\n",
    "        if v in ski_venues: return \"ski\"\n",
    "        if v in hwadam_venues: return \"hwadam\"\n",
    "        if v in lodging_venues: return \"lodging\"\n",
    "        return \"other\"\n",
    "    group_name = venue.map(_map_group)\n",
    "    out['venue_group_label'] = group_name.map({\"ski\":0,\"hwadam\":1,\"lodging\":2,\"other\":-1}).astype('int8')\n",
    "\n",
    "    cats = pd.Categorical(group_name, categories=[\"ski\",\"hwadam\",\"lodging\"])\n",
    "    dummies = pd.get_dummies(cats, prefix='grp')\n",
    "    for c in ['grp_ski','grp_hwadam','grp_lodging']:\n",
    "        if c not in dummies: dummies[c] = 0\n",
    "    return pd.concat([out, dummies[['grp_ski','grp_hwadam','grp_lodging']].astype('int8')], axis=1)\n",
    "\n",
    "# ----------------------- Build features & master df -------------------------\n",
    "def build_feature_frames_in_memory_all():\n",
    "    key_col, target = CANON_KEY, \"매출수량\"\n",
    "    price_raw  = read_csv_safe(PRICE_PATH)\n",
    "    price_slim = preprocess_price(price_raw)\n",
    "\n",
    "    # Train\n",
    "    train = read_csv_safe(TRAIN_PATH, parse_dates=[\"영업일자\"])\n",
    "    train = normalize_key_column(train)\n",
    "    assert key_col in train.columns, f\"키 없음: {list(train.columns)}\"\n",
    "    train[target] = pd.to_numeric(train[target], errors='coerce').fillna(0).clip(lower=0)\n",
    "    train = train.merge(price_slim, on=key_col, how=\"left\")\n",
    "    train_ratio = build_room_ratio(TRAIN_room_PATH, room_type_meta)\n",
    "    train = attach_ratio(train, train_ratio)\n",
    "    train_ski  = read_csv_safe(TRAIN_ski_PATH)\n",
    "    train = attach_daily_ski(train, train_ski)\n",
    "    train = train.sort_values([key_col, \"영업일자\"])\n",
    "    train = finalize_features(train)\n",
    "    train_X = add_venue_group_features(train.drop(columns=[target], errors=\"ignore\"), col=key_col, keep_venue_name=True)\n",
    "    train_y = train[target].copy()\n",
    "\n",
    "    # Test\n",
    "    test_X_dict = {}\n",
    "    for tf in sorted(glob.glob(os.path.join(TEST_DIR, \"TEST_*.csv\"))):\n",
    "        sid = Path(tf).stem.split(\"_\")[-1]\n",
    "        tdf = read_csv_safe(tf, parse_dates=[\"영업일자\"])\n",
    "        tdf = normalize_key_column(tdf)\n",
    "        assert key_col in tdf.columns, f\"{tf} 키 없음: {list(tdf.columns)}\"\n",
    "        tdf[target] = pd.to_numeric(tdf.get(target, 0), errors='coerce').fillna(0).clip(lower=0)\n",
    "        tdf = tdf.merge(price_slim, on=key_col, how=\"left\")\n",
    "        trot = build_room_ratio(os.path.join(TEST_meta_DIR, f\"TEST_room_{sid}.csv\"), room_type_meta)\n",
    "        tdf  = attach_ratio(tdf, trot)\n",
    "        tsdf = read_csv_safe(os.path.join(TEST_meta_DIR, f\"TEST_ski_{sid}.csv\"))\n",
    "        tdf  = attach_daily_ski(tdf, tsdf)\n",
    "        tdf  = tdf.sort_values([key_col, \"영업일자\"])\n",
    "        tdf  = finalize_features(tdf)\n",
    "        test_X = add_venue_group_features(tdf.drop(columns=[target], errors=\"ignore\"), col=key_col, keep_venue_name=True)\n",
    "        test_X_dict[f\"TEST_{sid}\"] = test_X\n",
    "\n",
    "    # Align test columns to train_X\n",
    "    base_cols = list(train_X.columns)\n",
    "    for k, df in test_X_dict.items():\n",
    "        for c in base_cols:\n",
    "            if c not in df.columns:\n",
    "                df[c] = 0 if c.startswith(\"grp_\") else np.nan\n",
    "        test_X_dict[k] = df[base_cols]\n",
    "\n",
    "    return train_X, train_y, test_X_dict\n",
    "\n",
    "train_X, train_y, testX_dict = build_feature_frames_in_memory_all()\n",
    "\n",
    "# Make master train df (with target)\n",
    "train_df = train_X.copy()\n",
    "train_df[\"매출수량\"] = pd.to_numeric(train_y, errors='coerce').fillna(0).clip(lower=0)\n",
    "\n",
    "# --------------- Classic helpers for store/menu/category --------------------\n",
    "def split_store_menu(df):\n",
    "    df = df.copy()\n",
    "    if ('영업장명' in df.columns) and ('메뉴명' in df.columns):\n",
    "        return df\n",
    "    df = normalize_key_column(df)\n",
    "    sp = df[CANON_KEY].astype(str).str.split('_', n=1, expand=True)\n",
    "    df['영업장명'] = sp[0].str.strip()\n",
    "    df['메뉴명']  = sp[1].str.strip() if sp.shape[1] > 1 else \"\"\n",
    "    return df\n",
    "\n",
    "def categorize_menu(menu_name: str) -> str:\n",
    "    name = str(menu_name)\n",
    "    if any(k in name for k in ['연회장','Conference','Grand Ballroom','Convention','OPUS','Hall','(단체)','무제한','단체식']): return '행사/연회'\n",
    "    if any(k in name for k in ['대여','대여료','렌탈','잔디그늘집','의자','룸 이용료','룸']): return '대여'\n",
    "    if any(k in name for k in ['막걸리','소주','참이슬','처음처럼','카스','테라','하이네켄','버드와이저','스텔라','와인','글라스와인','Gls','Beer','맥주','하이볼','칵테일','복분자']): return '주류'\n",
    "    if any(k in name for k in ['콜라','코카콜라','제로','스프라이트','에이드','아메리카노','라떼','아이스티','생수','커피','식혜','미숫가루','메밀미숫가루']): return '음료'\n",
    "    if any(k in name for k in ['공깃밥','주먹밥','야채추가','빵 추가','면 사리','파스타면 추가','쌈야채','친환경 접시','접시']): return '사이드'\n",
    "    if any(k in name for k in ['패키지','Platter','세트','Open Food','오픈푸드','브런치']): return '세트/패키지'\n",
    "    if any(k in name for k in ['비빔밥','찌개','국밥','탕','전골','돈까스','리조또','파스타','스파게티','떡볶이','우동','짬뽕','짜장','삼겹','양갈비','샐러드','핫도그','치즈','우거지','된장','냉면','물냉면','비빔냉면','김치찌개','떡갈비','불고기','갈비탕','랍스타','스튜','AUS','한우']): return '메인'\n",
    "    if any(k in name for k in ['수저','젓가락','컵','종이컵','소주컵','일회용','접시','쌈장','허브솔트','햇반','라면사리','샷 추가']): return '소모품'\n",
    "    return '기타'\n",
    "\n",
    "def make_zero_streak(x):\n",
    "    z = (np.asarray(x) <= 0).astype(np.int32)\n",
    "    out = np.zeros_like(z, dtype=np.float32); run = 0\n",
    "    for i, zz in enumerate(z):\n",
    "        run = run + 1 if zz == 1 else 0\n",
    "        out[i] = run\n",
    "    return out\n",
    "\n",
    "# ----------------------- Label encoders -------------------------------------\n",
    "train_df = split_store_menu(train_df)\n",
    "train_df['카테고리'] = train_df['메뉴명'].apply(categorize_menu)\n",
    "\n",
    "store_le = LabelEncoder(); menu_le = LabelEncoder(); cat_le = LabelEncoder()\n",
    "train_df['영업장명_le'] = store_le.fit_transform(train_df['영업장명'])\n",
    "train_df['메뉴명_le']   = menu_le.fit_transform(train_df['메뉴명'])\n",
    "train_df['카테고리_le'] = cat_le.fit_transform(train_df['카테고리'])\n",
    "n_stores = len(store_le.classes_); n_menus = len(menu_le.classes_); n_cats = len(cat_le.classes_)\n",
    "n_dow, n_season = 7, 4\n",
    "\n",
    "# ----------------------- EXTRA_COLS & scaling -------------------------------\n",
    "EXTRA_COLS = [\n",
    "    '평균판매금액','cooking_ratio','1일내장객',\n",
    "    '일','주차','공휴일여부','월_sin','월_cos','요일_sin','요일_cos',\n",
    "    'is_weekend','전일_공휴일','익일_공휴일','휴일전날','휴일다음날','연휴중여부',\n",
    "    'wd_mean_local_28','lag_7','lag_14',\n",
    "    'roll_mean_1','roll_mean_3','roll_mean_7','roll_mean_14','roll_mean_28',\n",
    "    'roll_std_3','roll_std_7','roll_std_14','roll_std_28',\n",
    "    'grp_ski','grp_hwadam','grp_lodging',\n",
    "]\n",
    "# Ensure numeric + fill NA\n",
    "train_df[EXTRA_COLS] = train_df[EXTRA_COLS].apply(pd.to_numeric, errors='coerce')\n",
    "train_df[EXTRA_COLS] = train_df[EXTRA_COLS].fillna(train_df[EXTRA_COLS].median())\n",
    "extra_scaler = MinMaxScaler()\n",
    "train_df[EXTRA_COLS] = extra_scaler.fit_transform(train_df[EXTRA_COLS])\n",
    "\n",
    "# ----------------------- Final-round metric ---------------------------------\n",
    "def _mask_nonzero_y(a, p):\n",
    "    a = np.asarray(a, dtype=float); p = np.asarray(p, dtype=float)\n",
    "    m = (a != 0) & np.isfinite(a) & np.isfinite(p)\n",
    "    return a[m], p[m]\n",
    "\n",
    "def smape_comp(a, p, eps=1e-9):\n",
    "    a, p = _mask_nonzero_y(a, p)\n",
    "    if a.size == 0: return 0.0\n",
    "    return float(np.mean(2*np.abs(a-p)/(np.abs(a)+np.abs(p)+eps)))\n",
    "\n",
    "def nmae_comp(a, p, eps=1e-9):\n",
    "    a, p = _mask_nonzero_y(a, p)\n",
    "    if a.size == 0: return 0.0\n",
    "    return float(np.mean(np.abs(a-p)) / (np.mean(a)+eps))\n",
    "\n",
    "def nrmse_comp(a, p, eps=1e-9):\n",
    "    a, p = _mask_nonzero_y(a, p)\n",
    "    if a.size == 0: return 0.0\n",
    "    return float(np.sqrt(np.mean((a-p)**2)) / (np.mean(a)+eps))\n",
    "\n",
    "def r2_comp(a, p):\n",
    "    a, p = _mask_nonzero_y(a, p)\n",
    "    if a.size < 2: return 0.0\n",
    "    ca=a-a.mean(); cp=p-p.mean()\n",
    "    denom = np.linalg.norm(ca)*np.linalg.norm(cp)\n",
    "    if denom==0: return 0.0\n",
    "    r = float(np.dot(ca, cp)/denom)\n",
    "    return r*r\n",
    "\n",
    "def final_score(a, p):\n",
    "    A = 1.0 - smape_comp(a, p)/2.0           # sMAPE in [0,2] -> normalized\n",
    "    B = 1.0 - min(1.0, nmae_comp(a, p))\n",
    "    C = 1.0 - min(1.0, nrmse_comp(a, p))\n",
    "    D = r2_comp(a, p)\n",
    "    return 0.25*(A+B+C+D)\n",
    "\n",
    "# ----------------------- Windows -------------------------------------------\n",
    "def build_train_val_windows(df):\n",
    "    train_items, val_items = [], []\n",
    "    for (st, mn), g in df.groupby(['영업장명','메뉴명'], sort=True):\n",
    "        g = g.sort_values('영업일자')\n",
    "        if len(g) < LOOKBACK + HORIZON: continue\n",
    "\n",
    "        vals = g['매출수량'].astype(np.float32).values\n",
    "        vals = np.maximum(vals, 0.0)\n",
    "\n",
    "        d = pd.to_datetime(g['영업일자'])\n",
    "        dow = d.dt.dayofweek.astype(np.int64).values\n",
    "        season = d.dt.month.map(lambda m: 0 if m in [12,1,2] else 1 if m in [3,4,5] else 2 if m in [6,7,8] else 3).astype(np.int64).values\n",
    "\n",
    "        extras = g[EXTRA_COLS].to_numpy(np.float32)\n",
    "\n",
    "        sid  = int(store_le.transform([st])[0])\n",
    "        mid  = int(menu_le.transform([mn])[0])\n",
    "        cid  = int(cat_le.transform([g.iloc[0]['카테고리']])[0])\n",
    "\n",
    "        total = len(g) - LOOKBACK - HORIZON + 1\n",
    "        for start in range(total):\n",
    "            s, e_in, e_all = start, start+LOOKBACK, start+LOOKBACK+HORIZON\n",
    "            x_raw  = np.maximum(vals[s:e_in], 0.0)\n",
    "            y_raw  = np.maximum(vals[e_in:e_all], 0.0)\n",
    "            x_dow  = dow[s:e_in]\n",
    "            x_sea  = season[s:e_in]\n",
    "            x_zero = make_zero_streak(x_raw)\n",
    "            x_extra = extras[s:e_in, :]\n",
    "\n",
    "            item = {\n",
    "                'x_vals': np.log1p(x_raw),\n",
    "                'x_dow': x_dow, 'x_sea': x_sea,\n",
    "                'x_zero': x_zero, 'x_extra': x_extra,\n",
    "                'store': sid, 'menu': mid, 'cat': cid,\n",
    "                'y_val': np.log1p(y_raw),\n",
    "                'y_mask': (y_raw > 0).astype(np.float32),\n",
    "                'col': f\"{st}_{mn}\"\n",
    "            }\n",
    "            (val_items if start == total-1 else train_items).append(item)\n",
    "\n",
    "    if len(val_items) == 0 and len(train_items) > 0:\n",
    "        cut = min(1024, len(train_items)//5 if len(train_items) >= 5 else 1)\n",
    "        val_items = train_items[-cut:]; train_items = train_items[:-cut]\n",
    "    return train_items, val_items\n",
    "\n",
    "train_items, val_items = build_train_val_windows(train_df)\n",
    "\n",
    "# ----------------------- Dataset/DataLoader ---------------------------------\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, items, has_target=True):\n",
    "        self.items = items; self.has_target = has_target\n",
    "    def __len__(self): return len(self.items)\n",
    "    def __getitem__(self, i):\n",
    "        it = self.items[i]\n",
    "        x_vals=torch.tensor(it['x_vals'],dtype=torch.float32)\n",
    "        x_dow =torch.tensor(it['x_dow'], dtype=torch.long)\n",
    "        x_sea =torch.tensor(it['x_sea'], dtype=torch.long)\n",
    "        x_zero=torch.tensor(it['x_zero'],dtype=torch.float32)\n",
    "        x_extra=torch.tensor(it['x_extra'],dtype=torch.float32)\n",
    "        # sanitize any weird nums just in case\n",
    "        x_vals = torch.nan_to_num(x_vals, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        x_zero = torch.nan_to_num(x_zero, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        x_extra= torch.nan_to_num(x_extra,nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        store =torch.tensor(it['store'], dtype=torch.long)\n",
    "        menu  =torch.tensor(it['menu'], dtype=torch.long)\n",
    "        cat   =torch.tensor(it['cat'], dtype=torch.long)\n",
    "        if self.has_target:\n",
    "            y_val =torch.tensor(it['y_val'], dtype=torch.float32)\n",
    "            y_mask=torch.tensor(it['y_mask'],dtype=torch.float32)\n",
    "            y_val = torch.nan_to_num(y_val, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            return x_vals,x_dow,x_sea,x_zero,x_extra,store,menu,cat,y_val,y_mask,it['col']\n",
    "        else:\n",
    "            return x_vals,x_dow,x_sea,x_zero,x_extra,store,menu,cat,it['col']\n",
    "\n",
    "g = torch.Generator(); g.manual_seed(SEED)\n",
    "train_dl = DataLoader(SeqDataset(train_items, True), batch_size=BATCH, shuffle=True,\n",
    "                      num_workers=0, generator=g, persistent_workers=False, drop_last=False)\n",
    "val_dl   = DataLoader(SeqDataset(val_items, True),   batch_size=BATCH, shuffle=False,\n",
    "                      num_workers=0, generator=g, persistent_workers=False, drop_last=False)\n",
    "\n",
    "# ----------------------- Models --------------------------------------------\n",
    "class LSTMTwoHeadWithCat(nn.Module):\n",
    "    def __init__(self, n_stores, n_menus, n_cats, n_dow=7, n_season=4,\n",
    "                 hidden=128, emb_store=16, emb_menu=32, emb_cat=8, emb_dow=4, emb_sea=2,\n",
    "                 extra_dim=0, horizon=7):\n",
    "        super().__init__()\n",
    "        self.store_emb = nn.Embedding(n_stores, emb_store)\n",
    "        self.menu_emb  = nn.Embedding(n_menus,  emb_menu)\n",
    "        self.cat_emb   = nn.Embedding(n_cats,   emb_cat)\n",
    "        self.dow_emb   = nn.Embedding(n_dow,    emb_dow)\n",
    "        self.sea_emb   = nn.Embedding(n_season, emb_sea)\n",
    "        in_dim = (1 + 1) + extra_dim + emb_dow + emb_sea + emb_store + emb_menu + emb_cat\n",
    "        self.lstm = nn.LSTM(in_dim, hidden, batch_first=True)\n",
    "        self.cls_head = nn.Linear(hidden, horizon)\n",
    "        self.reg_head = nn.Sequential(nn.Linear(hidden, hidden), nn.ReLU(), nn.Linear(hidden, horizon))\n",
    "    def forward(self, x_vals, x_dow, x_sea, x_zero, x_extra, store, menu, cat):\n",
    "        B, L = x_vals.shape\n",
    "        x = torch.cat([\n",
    "            x_vals.unsqueeze(-1),\n",
    "            x_zero.unsqueeze(-1),\n",
    "            x_extra,\n",
    "            self.dow_emb(x_dow),\n",
    "            self.sea_emb(x_sea),\n",
    "            self.store_emb(store).unsqueeze(1).repeat(1,L,1),\n",
    "            self.menu_emb(menu).unsqueeze(1).repeat(1,L,1),\n",
    "            self.cat_emb(cat).unsqueeze(1).repeat(1,L,1),\n",
    "        ], dim=-1)\n",
    "        h, _ = self.lstm(x)\n",
    "        last = h[:, -1, :]\n",
    "        return self.cls_head(last), self.reg_head(last)\n",
    "\n",
    "class NHITSEncoder(nn.Module):\n",
    "    def __init__(self, L=28, latent=128, pools=(1,2,4), blocks=2, hidden=256, drop=0.0):\n",
    "        super().__init__()\n",
    "        assert all(L % r == 0 for r in pools)\n",
    "        self.L = L; self.pools = pools\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for r in pools:\n",
    "            Lp = L // r\n",
    "            stack = nn.ModuleList()\n",
    "            for _ in range(blocks):\n",
    "                stack.append(nn.Sequential(\n",
    "                    nn.Linear(Lp, hidden), nn.ReLU(),\n",
    "                    nn.Dropout(drop),\n",
    "                    nn.Linear(hidden, Lp)\n",
    "                ))\n",
    "            self.blocks.append(stack)\n",
    "        self.head = nn.Linear(L, latent)\n",
    "    @staticmethod\n",
    "    def _fixed_avg_pool1d(x, kernel):\n",
    "        y = F.avg_pool1d(x.unsqueeze(1), kernel_size=kernel, stride=kernel, ceil_mode=False)\n",
    "        return y.squeeze(1)\n",
    "    @staticmethod\n",
    "    def _upsample_nearest(y, out_len):\n",
    "        z = F.interpolate(y.unsqueeze(1), size=out_len, mode='nearest')\n",
    "        return z.squeeze(1)\n",
    "    def forward(self, x):  # x: (B, L)\n",
    "        r = x\n",
    "        for r_scale, stack in zip(self.pools, self.blocks):\n",
    "            z = self._fixed_avg_pool1d(r, kernel=r_scale)\n",
    "            for blk in stack:\n",
    "                b = blk(z); z = z - b\n",
    "            r = r - self._upsample_nearest(b, self.L)\n",
    "        return self.head(r)\n",
    "\n",
    "class NHITSTwoHeadWithCat(nn.Module):\n",
    "    def __init__(self, n_stores, n_menus, n_cats, n_dow=7, n_season=4,\n",
    "                 emb_store=16, emb_menu=32, emb_cat=8, emb_dow=4, emb_sea=2,\n",
    "                 extra_dim=0, latent=128, horizon=7):\n",
    "        super().__init__()\n",
    "        self.store_emb = nn.Embedding(n_stores, emb_store)\n",
    "        self.menu_emb  = nn.Embedding(n_menus,  emb_menu)\n",
    "        self.cat_emb   = nn.Embedding(n_cats,   emb_cat)\n",
    "        self.dow_emb   = nn.Embedding(n_dow,    emb_dow)\n",
    "        self.sea_emb   = nn.Embedding(n_season, emb_sea)\n",
    "        self.ctx_proj   = nn.Linear(emb_dow + emb_sea + emb_store + emb_menu + emb_cat, 1)\n",
    "        self.extra_proj = nn.Linear(1, 1)                 # zero-streak scalar\n",
    "        self.extra_num_proj = nn.Linear(extra_dim, 1)     # numeric extras → scalar\n",
    "        self.enc       = NHITSEncoder(L=LOOKBACK, latent=latent, pools=(1,2,4), blocks=2, hidden=256, drop=0.0)\n",
    "        self.cls_head  = nn.Linear(latent, horizon)\n",
    "        self.reg_head  = nn.Sequential(nn.Linear(latent, latent), nn.ReLU(), nn.Linear(latent, horizon))\n",
    "    def forward(self, x_vals, x_dow, x_sea, x_zero, x_extra, store, menu, cat):\n",
    "        B, L = x_vals.shape\n",
    "        ctx = torch.cat([\n",
    "            self.dow_emb(x_dow), self.sea_emb(x_sea),\n",
    "            self.store_emb(store).unsqueeze(1).repeat(1,L,1),\n",
    "            self.menu_emb(menu).unsqueeze(1).repeat(1,L,1),\n",
    "            self.cat_emb(cat).unsqueeze(1).repeat(1,L,1),\n",
    "        ], dim=-1)\n",
    "        ctx_scalar   = self.ctx_proj(ctx).squeeze(-1)                 # (B,L)\n",
    "        extra_scalar = self.extra_proj(x_zero.unsqueeze(-1)).squeeze(-1)\n",
    "        extra_num_sc = self.extra_num_proj(x_extra).squeeze(-1)       # (B,L)\n",
    "        x_in = x_vals + 0.10*ctx_scalar + 0.05*extra_scalar + 0.05*extra_num_sc\n",
    "        h = self.enc(x_in)\n",
    "        return self.cls_head(h), self.reg_head(h)\n",
    "\n",
    "# ----------------------- Train/Eval (composite metric) ----------------------\n",
    "def evaluate_composite(model, dl, temperature=1.0):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for x_vals,x_dow,x_sea,x_zero,x_extra,store,menu,cat,y_val,y_mask,cols in dl:\n",
    "            x_vals,x_dow,x_sea = x_vals.to(DEVICE), x_dow.to(DEVICE), x_sea.to(DEVICE)\n",
    "            x_zero = x_zero.to(DEVICE); x_extra = x_extra.to(DEVICE)\n",
    "            store,menu,cat = store.to(DEVICE), menu.to(DEVICE), cat.to(DEVICE)\n",
    "            y_val = y_val.to(DEVICE)\n",
    "            logit, reg = model(x_vals,x_dow,x_sea,x_zero,x_extra,store,menu,cat)\n",
    "            prob = torch.sigmoid(logit/temperature)\n",
    "            pred = (torch.expm1(reg) * prob).clamp_min(1)\n",
    "            y_true.append(torch.expm1(y_val).cpu().numpy())\n",
    "            y_pred.append(pred.cpu().numpy())\n",
    "    yt = np.concatenate(y_true,0).ravel()\n",
    "    yp = np.concatenate(y_pred,0).ravel()\n",
    "    return final_score(yt, yp)\n",
    "\n",
    "def train_model(model, train_dl, val_dl, ckpt_path):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "    bce = nn.BCEWithLogitsLoss(); huber = nn.SmoothL1Loss()\n",
    "    best = -1e9; wait = 0\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        model.train(); run=0; n=0\n",
    "        for x_vals,x_dow,x_sea,x_zero,x_extra,store,menu,cat,y_val,y_mask,_ in train_dl:\n",
    "            x_vals,x_dow,x_sea = x_vals.to(DEVICE), x_dow.to(DEVICE), x_sea.to(DEVICE)\n",
    "            x_zero = x_zero.to(DEVICE); x_extra = x_extra.to(DEVICE)\n",
    "            store,menu,cat = store.to(DEVICE), menu.to(DEVICE), cat.to(DEVICE)\n",
    "            y_val,y_mask = y_val.to(DEVICE), y_mask.to(DEVICE)\n",
    "\n",
    "            logit, reg = model(x_vals,x_dow,x_sea,x_zero,x_extra,store,menu,cat)\n",
    "            loss_cls = bce(logit, y_mask)\n",
    "            mask = (y_mask>0).float()\n",
    "            loss_reg = huber(reg*mask, y_val*mask) if mask.sum()>0 else torch.tensor(0.0, device=DEVICE)\n",
    "            loss = 1.0*loss_cls + 2.0*loss_reg\n",
    "\n",
    "            optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "            run += loss.item()*x_vals.size(0); n += x_vals.size(0)\n",
    "\n",
    "        valid_score = evaluate_composite(model, val_dl, temperature=1.0)\n",
    "        print(f\"[{ep:02d}/{EPOCHS}] train_loss={run/max(n,1):.4f}  valid_SCORE={valid_score:.6f}\")\n",
    "\n",
    "        if valid_score > best + 1e-12:\n",
    "            best = valid_score; wait = 0\n",
    "            torch.save(model.state_dict(), ckpt_path)\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= PATIENCE:\n",
    "                print(\"Early stopping.\"); break\n",
    "    print(f\"=> Best valid SCORE: {best:.6f}  ({ckpt_path})\")\n",
    "    sd = _load_state_dict_safe(ckpt_path, map_location=DEVICE)\n",
    "    model.load_state_dict(sd)\n",
    "    model.eval()\n",
    "    return best, model\n",
    "\n",
    "# ----------------------- Train both models ----------------------------------\n",
    "print(\"====================================\")\n",
    "print(\"       Training LSTM Model\")\n",
    "print(\"====================================\")\n",
    "lstm = LSTMTwoHeadWithCat(n_stores, n_menus, n_cats, n_dow, n_season,\n",
    "                          extra_dim=len(EXTRA_COLS), horizon=HORIZON).to(DEVICE)\n",
    "_, lstm = train_model(lstm, train_dl, val_dl, 'best_lstm.pt')\n",
    "\n",
    "print(\"====================================\")\n",
    "print(\"       Training N-HiTS Model\")\n",
    "print(\"====================================\")\n",
    "nhits = NHITSTwoHeadWithCat(n_stores, n_menus, n_cats, n_dow, n_season,\n",
    "                            extra_dim=len(EXTRA_COLS), latent=128, horizon=HORIZON).to(DEVICE)\n",
    "_, nhits = train_model(nhits, train_dl, val_dl, 'best_nhits.pt')\n",
    "\n",
    "# ----------------------- Temperature tuning ---------------------------------\n",
    "def tune_temperature(model, dl, grid):\n",
    "    scores = []\n",
    "    for t in grid:\n",
    "        s = evaluate_composite(model, dl, temperature=t)\n",
    "        scores.append((s, t))\n",
    "    best_s, best_t = max(scores, key=lambda x: x[0])\n",
    "    return best_t, best_s, scores\n",
    "\n",
    "BEST_TEMP_LSTM, LSTM_VAL_AT_BEST_T, _ = tune_temperature(lstm, val_dl, TEMP_GRID)\n",
    "BEST_TEMP_NHIT, NHIT_VAL_AT_BEST_T, _ = tune_temperature(nhits, val_dl, TEMP_GRID)\n",
    "print(f\"[Temp Tuning] LSTM best τ={BEST_TEMP_LSTM} → valid_SCORE={LSTM_VAL_AT_BEST_T:.6f}\")\n",
    "print(f\"[Temp Tuning] N-HiTS best τ={BEST_TEMP_NHIT} → valid_SCORE={NHIT_VAL_AT_BEST_T:.6f}\")\n",
    "\n",
    "# ----------------------- Validation blending --------------------------------\n",
    "@torch.no_grad()\n",
    "def collect_val_preds(model, dl, temperature=1.0):\n",
    "    model.eval()\n",
    "    col_list, y_list, p_list = [], [], []\n",
    "    for x_vals,x_dow,x_sea,x_zero,x_extra,store,menu,cat,y_val,y_mask,cols in dl:\n",
    "        x_vals,x_dow,x_sea = x_vals.to(DEVICE), x_dow.to(DEVICE), x_sea.to(DEVICE)\n",
    "        x_zero = x_zero.to(DEVICE); x_extra = x_extra.to(DEVICE)\n",
    "        store,menu,cat = store.to(DEVICE), menu.to(DEVICE), cat.to(DEVICE)\n",
    "        logit, reg = model(x_vals,x_dow,x_sea,x_zero,x_extra,store,menu,cat)\n",
    "        prob = torch.sigmoid(logit/temperature)\n",
    "        pred = (torch.expm1(reg) * prob).clamp_min(1)\n",
    "        col_list += list(cols)\n",
    "        y_list.append(torch.expm1(y_val).cpu().numpy())\n",
    "        p_list.append(pred.cpu().numpy())\n",
    "    y = np.concatenate(y_list,0); p = np.concatenate(p_list,0)\n",
    "    return np.array(col_list), y, p\n",
    "\n",
    "cols_v, y_v, pL_v = collect_val_preds(lstm, val_dl, temperature=BEST_TEMP_LSTM)\n",
    "_,     _, pN_v    = collect_val_preds(nhits, val_dl, temperature=BEST_TEMP_NHIT)\n",
    "\n",
    "def col_to_groupkey(colname):\n",
    "    store, menu = colname.split('_', 1)\n",
    "    cat = categorize_menu(menu)\n",
    "    if GROUPING == 'store': return store\n",
    "    elif GROUPING == 'category': return cat\n",
    "    elif GROUPING == 'store_category': return f\"{store}_{cat}\"\n",
    "    else: return (store, cat)\n",
    "\n",
    "keys_v = np.array([col_to_groupkey(c) for c in cols_v])\n",
    "\n",
    "def composite_arrays(y_true, yL, yN, a):\n",
    "    yp = a*yL + (1-a)*yN\n",
    "    return final_score(y_true.ravel(), yp.ravel())\n",
    "\n",
    "global_blend_score = composite_arrays(y_v, pL_v, pN_v, GLOBAL_ALPHA)\n",
    "print(f\"[Validation SCORE] LSTM(τ*)={LSTM_VAL_AT_BEST_T:.6f}  N-HiTS(τ*)={NHIT_VAL_AT_BEST_T:.6f}  Global Blend a=0.80: {global_blend_score:.6f}\")\n",
    "\n",
    "group_alpha = {}\n",
    "for g in np.unique(keys_v):\n",
    "    idx = np.where(keys_v == g)[0]\n",
    "    if idx.size < MIN_ITEMS_PER_GROUP: continue\n",
    "    y = y_v[idx]; l = pL_v[idx]; n = pN_v[idx]\n",
    "    best_s = -1e9; best_a = GLOBAL_ALPHA\n",
    "    for a in ALPHAS_GRID:\n",
    "        s = composite_arrays(y, l, n, a)\n",
    "        if s > best_s:\n",
    "            best_s, best_a = s, a\n",
    "    group_alpha[g] = best_a\n",
    "print(f\"Tuned groups (alpha): {len(group_alpha)} / {len(np.unique(keys_v))}\")\n",
    "\n",
    "def apply_group_blend(cols, pL, pN, default_a=GLOBAL_ALPHA):\n",
    "    out = np.zeros_like(pL)\n",
    "    for i, c in enumerate(cols):\n",
    "        gkey = col_to_groupkey(c)\n",
    "        a = group_alpha.get(gkey, default_a)\n",
    "        out[i] = a * pL[i] + (1-a) * pN[i]\n",
    "    return out\n",
    "\n",
    "p_group = apply_group_blend(cols_v, pL_v, pN_v, GLOBAL_ALPHA)\n",
    "group_blend_score = final_score(y_v.ravel(), p_group.ravel())\n",
    "print(\"======== Validation Summary ========\")\n",
    "print(f\"LSTM (best τ={BEST_TEMP_LSTM}): {LSTM_VAL_AT_BEST_T:.6f}\")\n",
    "print(f\"N-HiTS (best τ={BEST_TEMP_NHIT}): {NHIT_VAL_AT_BEST_T:.6f}\")\n",
    "print(f\"Global Blend (α=0.80): {global_blend_score:.6f}\")\n",
    "print(f\"Group Blend (tuned α): {group_blend_score:.6f}\")\n",
    "\n",
    "# ----------------------- Test inference ------------------------------------\n",
    "def preprocess_test_build_features(raw_df: pd.DataFrame, sid_tag: str) -> pd.DataFrame:\n",
    "    key_col = CANON_KEY; target=\"매출수량\"\n",
    "    price_raw  = read_csv_safe(PRICE_PATH)\n",
    "    price_slim = preprocess_price(price_raw)\n",
    "    df = normalize_key_column(raw_df)\n",
    "    df[target] = pd.to_numeric(df.get(target, 0), errors='coerce').fillna(0).clip(lower=0)\n",
    "    df = df.merge(price_slim, on=key_col, how=\"left\")\n",
    "\n",
    "    trot = build_room_ratio(os.path.join(TEST_meta_DIR, f\"TEST_room_{sid_tag}.csv\"), room_type_meta)\n",
    "    df  = attach_ratio(df, trot)\n",
    "\n",
    "    tsdf = read_csv_safe(os.path.join(TEST_meta_DIR, f\"TEST_ski_{sid_tag}.csv\"))\n",
    "    df  = attach_daily_ski(df, tsdf)\n",
    "\n",
    "    df = df.sort_values([key_col, \"영업일자\"])\n",
    "    df = finalize_features(df)\n",
    "    dfX = add_venue_group_features(df.drop(columns=[target], errors=\"ignore\"), col=key_col, keep_venue_name=True)\n",
    "\n",
    "    # numeric extras\n",
    "    dfX[EXTRA_COLS] = dfX[EXTRA_COLS].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "    dfX[EXTRA_COLS] = extra_scaler.transform(dfX[EXTRA_COLS])\n",
    "    dfX[target] = pd.to_numeric(df[target], errors='coerce').fillna(0).clip(lower=0)\n",
    "    return dfX\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict7(model, g_seq, sid, mid, cid, temperature=1.0):\n",
    "    g_seq = g_seq.sort_values('영업일자')\n",
    "    raw  = g_seq['매출수량'].values.astype(np.float32) if '매출수량' in g_seq.columns else np.zeros(LOOKBACK, np.float32)\n",
    "    raw  = np.maximum(raw, 0.0)\n",
    "    vals = np.log1p(raw)\n",
    "    d    = pd.to_datetime(g_seq['영업일자'])\n",
    "    dows = d.dt.dayofweek.astype(np.int64).values\n",
    "    seas = d.dt.month.map(lambda m: 0 if m in [12,1,2] else 1 if m in [3,4,5] else 2 if m in [6,7,8] else 3).astype(np.int64).values\n",
    "    x_extra = g_seq[EXTRA_COLS].to_numpy(np.float32)\n",
    "    if len(vals) != LOOKBACK:\n",
    "        return np.ones(HORIZON, dtype=np.float32)\n",
    "    x_zero = make_zero_streak(raw)\n",
    "\n",
    "    x_vals = torch.tensor(vals[None,:], dtype=torch.float32).to(DEVICE)\n",
    "    x_dow  = torch.tensor(dows[None,:], dtype=torch.long).to(DEVICE)\n",
    "    x_sea  = torch.tensor(seas[None,:], dtype=torch.long).to(DEVICE)\n",
    "    x_zero = torch.tensor(x_zero[None,:], dtype=torch.float32).to(DEVICE)\n",
    "    x_extra= torch.tensor(x_extra[None,:,:], dtype=torch.float32).to(DEVICE)\n",
    "    store  = torch.tensor([sid], dtype=torch.long).to(DEVICE)\n",
    "    menu   = torch.tensor([mid], dtype=torch.long).to(DEVICE)\n",
    "    cat    = torch.tensor([cid], dtype=torch.long).to(DEVICE)\n",
    "    logit, reg = model(x_vals,x_dow,x_sea,x_zero,x_extra,store,menu,cat)\n",
    "    prob = torch.sigmoid(logit/temperature)\n",
    "    pred = (torch.expm1(reg) * prob).clamp_min(1)\n",
    "    return pred.cpu().numpy().ravel()\n",
    "\n",
    "def group_key_from_names(store, menu):\n",
    "    cat = categorize_menu(menu)\n",
    "    if GROUPING == 'store': return store\n",
    "    elif GROUPING == 'category': return cat\n",
    "    else: return f\"{store}_{cat}\"\n",
    "\n",
    "# ---- Build submission (long & wide) ----\n",
    "sub_rows = []\n",
    "if os.path.exists(SAMPLE_PATH):\n",
    "    for path in sorted(glob.glob(os.path.join(TEST_DIR, \"TEST_*.csv\"))):\n",
    "        t_raw = read_csv_safe(path, parse_dates=[\"영업일자\"])\n",
    "        t_raw = normalize_key_column(t_raw)\n",
    "        tag   = re.search(r'TEST_(\\d+)', os.path.basename(path)).group(1)\n",
    "        t_df  = preprocess_test_build_features(t_raw, sid_tag=tag)\n",
    "\n",
    "        # encode ids\n",
    "        t_df = split_store_menu(t_df)\n",
    "        t_df['카테고리'] = t_df['메뉴명'].apply(categorize_menu)\n",
    "        def map_safe(le, values):\n",
    "            mp = {c:i for i,c in enumerate(le.classes_)}\n",
    "            return np.array([mp.get(v, 0) for v in values], dtype=np.int64)\n",
    "        t_df['영업장명_le'] = map_safe(store_le, t_df['영업장명'].values)\n",
    "        t_df['메뉴명_le']   = map_safe(menu_le,  t_df['메뉴명'].values)\n",
    "        t_df['카테고리_le'] = map_safe(cat_le,   t_df['카테고리'].values)\n",
    "\n",
    "        for (sid, mid, cid), g in t_df.groupby(['영업장명_le','메뉴명_le','카테고리_le'], sort=True):\n",
    "            g28 = g.sort_values('영업일자').tail(LOOKBACK)\n",
    "            if len(g28) < LOOKBACK: continue\n",
    "            yL = predict7(lstm, g28, sid, mid, cid, temperature=BEST_TEMP_LSTM)\n",
    "            yN = predict7(nhits, g28, sid, mid, cid, temperature=BEST_TEMP_NHIT)\n",
    "\n",
    "            store_name = g28.iloc[-1]['영업장명']\n",
    "            menu_name  = g28.iloc[-1]['메뉴명']\n",
    "            gkey = group_key_from_names(store_name, menu_name)\n",
    "            a = group_alpha.get(gkey, GLOBAL_ALPHA)\n",
    "            y = np.maximum(a*yL + (1-a)*yN, 1.0)\n",
    "\n",
    "            sm_name = f\"{store_name}_{menu_name}\"\n",
    "            for i, v in enumerate(y, 1):\n",
    "                sub_rows.append({'영업일자': f'TEST_{tag}+{i}일', CANON_KEY: sm_name, '매출수량': float(v)})\n",
    "else:\n",
    "    print(\"sample_submission.csv not found; skipped test inference.\")\n",
    "\n",
    "pred_long = pd.DataFrame(sub_rows)\n",
    "long_path = os.path.join(DATA_PATH, \"submission_groupblend_long.csv\")\n",
    "if len(pred_long):\n",
    "    pred_long.to_csv(long_path, index=False)\n",
    "    print(f\"[Saved] {long_path}  shape={pred_long.shape}\")\n",
    "else:\n",
    "    print(\"No predictions were generated. Check TEST_*.csv availability.\")\n",
    "\n",
    "def save_wide_aligned(sample_path, sub_long, out_path):\n",
    "    if not os.path.exists(sample_path): return False, None\n",
    "    sample = pd.read_csv(sample_path)\n",
    "    sample = sanitize_columns(sample)\n",
    "    if '영업일자' in sample.columns and sample.shape[1] > 3:\n",
    "        wide = sub_long.pivot_table(index='영업일자',\n",
    "                                    columns=CANON_KEY,\n",
    "                                    values='매출수량',\n",
    "                                    aggfunc='first')\n",
    "        items = list(sample.columns[1:]); dates = sample['영업일자']\n",
    "        wide = wide.reindex(index=dates, columns=items)\n",
    "        wide = wide.apply(pd.to_numeric, errors='coerce').fillna(1.0)\n",
    "        wide = np.clip(wide, 1.0, None)\n",
    "        out = pd.concat([sample[['영업일자']].reset_index(drop=True),\n",
    "                         pd.DataFrame(wide, columns=items).reset_index(drop=True)], axis=1)\n",
    "        out.to_csv(out_path, index=False)\n",
    "        return True, out.shape\n",
    "    return False, None\n",
    "\n",
    "saved_wide, wide_shape = save_wide_aligned(SAMPLE_PATH, pred_long, SUBMIT_PATH)\n",
    "if saved_wide:\n",
    "    print(f\"[Saved] {SUBMIT_PATH} (WIDE aligned) shape={wide_shape}\")\n",
    "else:\n",
    "    print(\"Sample is not WIDE or not found → only long CSV saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4d55e0",
   "metadata": {},
   "source": [
    "# 머신"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58eff1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 최종 피처 개수: 30\n",
      "['wd_mean_local_28', 'lag_14', 'lag_7', 'nz_ratio_28', 'roll_mean_1', 'roll_mean_28', 'roll_mean_3', 'roll_std_28', 'roll_std_3', 'roll_std_7', 'streak_pos', '일', '요일', '주차', '계절', '공휴일여부', '월_sin', '월_cos', '요일_sin', '요일_cos', 'is_weekend', '전일_공휴일', '익일_공휴일', '휴일전날', '휴일다음날', '연휴중여부', 'rfm_recency_train', 'rfm_freq_train', 'rfm_meanlead_train', 'rfm_avgqty_train']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 단일 셀: 메뉴별 매출수량 예측 (피처링: 날씨 + 시계열 + 시간/캘린더 + 메뉴카테고리 + RFM)\n",
    "# ============================================================\n",
    "\n",
    "# -------------------------\n",
    "# 0) 기본 설정/임포트\n",
    "# -------------------------\n",
    "import os, re, glob, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.display.float_format = \"{:.4f}\".format\n",
    "np.random.seed(42)\n",
    "\n",
    "try:\n",
    "    from IPython.display import display\n",
    "except Exception:\n",
    "    def display(x): print(x)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import holidays  # 공휴일\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1) 경로\n",
    "# ------------------------------------------------\n",
    "BASE_DIR = Path(\"../data\")\n",
    "TRAIN_DIR = BASE_DIR / \"train\"\n",
    "TEST_DIR  = BASE_DIR / \"test\"\n",
    "\n",
    "TRAIN_PATH        = TRAIN_DIR / \"train.csv\"\n",
    "SAMPLE_SUB_PATH   = BASE_DIR / \"sample_submission.csv\"\n",
    "TRAIN_META_DIR    = TRAIN_DIR / \"meta\"\n",
    "TEST_META_DIR     = TEST_DIR / \"meta\"\n",
    "TEST_GLOB         = sorted([p.name for p in TEST_DIR.glob(\"TEST_*.csv\")])\n",
    "\n",
    "WEATHER_DIR = Path(\"weather\")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 2) 유틸(평가지표·시간/캘린더·카테고리·시계열·클렌징)\n",
    "# ------------------------------------------------\n",
    "def group_smape_by_store_with_detail(df, actual_col='매출수량', pred_col='예측값', group_col='업장명'):\n",
    "    smape_per_store = {}\n",
    "    eps = 1e-9\n",
    "    for store, g in df.groupby(group_col):\n",
    "        gg = g[g[actual_col] != 0]\n",
    "        if len(gg)==0:\n",
    "            s = np.nan\n",
    "        else:\n",
    "            num = np.abs(gg[actual_col]-gg[pred_col])\n",
    "            den = np.abs(gg[actual_col]) + np.abs(gg[pred_col]) + eps\n",
    "            s = (2 * num / den).mean()\n",
    "        smape_per_store[store] = float(s) if np.isfinite(s) else np.nan\n",
    "    overall_mean = float(np.nanmean(list(smape_per_store.values()))) if smape_per_store else np.nan\n",
    "    return smape_per_store, overall_mean\n",
    "\n",
    "def smape_group_macro(df, actual_col='매출수량', pred_col='예측값', group_col='업장명'):\n",
    "    _, overall = group_smape_by_store_with_detail(df, actual_col, pred_col, group_col)\n",
    "    return overall\n",
    "\n",
    "# ---- 시간/캘린더/메뉴 카테고리 ----\n",
    "def add_time_features(df, date_col='영업일자'):\n",
    "    out = df.copy()\n",
    "    dt = out[date_col].dt\n",
    "    month = dt.month\n",
    "    out['일']   = dt.day\n",
    "    out['요일'] = dt.dayofweek\n",
    "    out['주차'] = dt.isocalendar().week.astype('int32')\n",
    "    out['계절'] = month.map(lambda m: 0 if m in [12, 1, 2]\n",
    "                                      else 1 if m in [3, 4, 5]\n",
    "                                      else 2 if m in [6, 7, 8]\n",
    "                                      else 3)\n",
    "    kr_years = dt.year.unique()\n",
    "    kr_holidays = holidays.KR(years=kr_years)\n",
    "    # 타입 보정: date 비교 사용\n",
    "    out['공휴일여부'] = out[date_col].dt.date.isin(kr_holidays).astype(int)\n",
    "    out['월_sin']   = np.sin(2*np.pi*month/12)\n",
    "    out['월_cos']   = np.cos(2*np.pi*month/12)\n",
    "    out['요일_sin'] = np.sin(2*np.pi*out['요일']/7)\n",
    "    out['요일_cos'] = np.cos(2*np.pi*out['요일']/7)\n",
    "    return out\n",
    "\n",
    "def add_calendar_context_features(df: pd.DataFrame, date_col: str = '영업일자') -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    d = out[date_col].dt\n",
    "    if 'is_weekend' not in out.columns:\n",
    "        out['is_weekend'] = (d.dayofweek >= 5).astype(int)\n",
    "\n",
    "    years = sorted(out[date_col].dt.year.unique())\n",
    "    kr_hdays = set(holidays.KR(years=years).keys())\n",
    "    prev_day = (out[date_col] - pd.Timedelta(days=1)).dt.date\n",
    "    next_day = (out[date_col] + pd.Timedelta(days=1)).dt.date\n",
    "    out['전일_공휴일'] = prev_day.map(lambda x: int(x in kr_hdays))\n",
    "    out['익일_공휴일'] = next_day.map(lambda x: int(x in kr_hdays))\n",
    "\n",
    "    if '공휴일여부' in out.columns:\n",
    "        out['휴일전날']   = ((out['공휴일여부'] == 0) & (out['익일_공휴일'] == 1)).astype(int)\n",
    "        out['휴일다음날'] = ((out['공휴일여부'] == 0) & (out['전일_공휴일'] == 1)).astype(int)\n",
    "        out['연휴중여부'] = (((out['전일_공휴일'] == 1) | (out['익일_공휴일'] == 1)) & (out['공휴일여부'] == 1)).astype(int)\n",
    "    else:\n",
    "        out['휴일전날']   = (out['익일_공휴일'] == 1).astype(int)\n",
    "        out['휴일다음날'] = (out['전일_공휴일'] == 1).astype(int)\n",
    "        out['연휴중여부'] = ((out['전일_공휴일'] == 1) | (out['익일_공휴일'] == 1)).astype(int)\n",
    "    return out\n",
    "\n",
    "def classify_menu_features(df, col='메뉴명'):\n",
    "    df = df.copy()\n",
    "    name_col = df[col].astype(str).str.lower().str.replace(\" \", \"\", regex=False)\n",
    "    categories = {\n",
    "        '일회용품': ['일회용', '접시', '수저', '종이컵', '컵'],\n",
    "        'BBQ' : ['돈육구이', '킬바사소세지', 'bbq', '양갈비', '삼겹살추가', '200g', '본삼겹'],\n",
    "        '한식': ['김치', '된장', '비빔', '국밥', '찌개', '순대', '파전', '돌솥', '설렁탕', '해장국', '한우', '냉면', '갱시기', '양지탕', '갈비', '골뱅이', '닭발','정식'],\n",
    "        '중식': ['짜장', '짬뽕', '마라','볶음밥'],\n",
    "        '양식': ['파스타', '돈까스', '샐러드', '스테이크', '리조또', '피자', '햄버거', '치즈프라이', '알리오', '까르보나라', '스파게티', '플래터', '브런치', '패키지'],\n",
    "        '간식' : ['떡볶이', '신라면', '사발면', '페스츄리소시지', '꼬치어묵', '핫도그', '우동'],\n",
    "        '음료': ['아메리카노', '라떼', '에이드', '주스', '식혜', '차', '커피', '티', '콜라', 'coffee', '스프라이트', '미숫가루', '생수'],\n",
    "        '주류': ['소주', '카스', '참이슬', '막걸리', '하이네켄', '버드와이저', '맥주', '테라', '칵테일', 'wine', '하이볼', '토닉', 'gls', 'beer', '처음처럼', '와인', '미션서드카베르네쉬라', '스텔라', '샷'],\n",
    "        '디저트': ['아이스크림', '쿠키', '디저트', '케이크', '스크림'],\n",
    "        '식자재': ['공깃밥', '햇반', '쌈장', '소스', '사리', '라면사리', '쌈야채', '주먹밥', '허브솔트', '빵추가', '야채추가'],\n",
    "        '대여': ['대여', '이용료', '의자', '룸', 'conference', 'hall', 'ballroom', 'opus']\n",
    "    }\n",
    "    def map_category(name):\n",
    "        for cat, keywords in categories.items():\n",
    "            for kw in keywords:\n",
    "                if kw in name: return cat\n",
    "        return '기타'\n",
    "    df['menu_category'] = name_col.apply(map_category)\n",
    "    return df\n",
    "\n",
    "# ---- 시계열 ----\n",
    "def add_filtered_ts_features(\n",
    "    df: pd.DataFrame,\n",
    "    group_cols=('영업장명_메뉴명',),\n",
    "    date_col: str = '영업일자',\n",
    "    target_col: str = '매출수량',\n",
    "    lags=(7, 14),\n",
    "    roll_mean_windows=(1, 3, 7, 14, 28),\n",
    "    roll_std_windows=(3, 7, 14, 28),\n",
    "    use_nz_ratio_28=True,\n",
    "    use_trend_7_28=False,\n",
    "    use_lag7_diff14=False,\n",
    "    use_streak_pos=True,\n",
    "):\n",
    "    if isinstance(group_cols, str):\n",
    "        group_cols = (group_cols,)\n",
    "    out = df.sort_values(list(group_cols) + [date_col]).copy()\n",
    "    g = out.groupby(list(group_cols))[target_col]\n",
    "    prev = g.shift(1)\n",
    "    keys = [out[c] for c in group_cols]\n",
    "    for k in lags:\n",
    "        out[f'lag_{k}'] = g.shift(k)\n",
    "    for w in roll_mean_windows:\n",
    "        out[f'roll_mean_{w}'] = (\n",
    "            prev.groupby(keys).rolling(w, min_periods=1).mean()\n",
    "                .reset_index(level=list(range(len(group_cols))), drop=True)\n",
    "        )\n",
    "    for w in roll_std_windows:\n",
    "        out[f'roll_std_{w}'] = (\n",
    "            prev.groupby(keys).rolling(w, min_periods=2).std()\n",
    "                .reset_index(level=list(range(len(group_cols))), drop=True)\n",
    "        )\n",
    "    if use_nz_ratio_28:\n",
    "        nz = (prev > 0).astype(float)\n",
    "        out['nz_ratio_28'] = (\n",
    "            nz.groupby(keys).rolling(28, min_periods=1).mean()\n",
    "              .reset_index(level=list(range(len(group_cols))), drop=True)\n",
    "        )\n",
    "    if use_trend_7_28 and {'roll_mean_7','roll_mean_28'} <= set(out.columns):\n",
    "        out['trend_7_28'] = out['roll_mean_7'] / (out['roll_mean_28'] + 1e-8) - 1\n",
    "    if use_lag7_diff14 and {'lag_7','lag_14'} <= set(out.columns):\n",
    "        out['lag_7_diff_14'] = out['lag_7'] - out['lag_14']\n",
    "    if use_streak_pos:\n",
    "        prev_pos = (prev.fillna(0) > 0)\n",
    "        def _streak(arr_bool: pd.Series) -> pd.Series:\n",
    "            grp_key = (~arr_bool).cumsum()\n",
    "            return arr_bool.groupby(grp_key).cumsum()\n",
    "        out['streak_pos'] = prev_pos.groupby(keys).transform(_streak).fillna(0).astype('int32')\n",
    "    return out\n",
    "\n",
    "def _add_wd_mean_local_28(g: pd.DataFrame) -> pd.DataFrame:\n",
    "    g = g.sort_values('영업일자').copy()\n",
    "    s = pd.to_numeric(g['매출수량'], errors='coerce')\n",
    "    same_wd_mean = s.shift(7).rolling(4, min_periods=1).mean()\n",
    "    all_wd_mean  = s.shift(1).rolling(7, min_periods=1).mean()\n",
    "    wd = same_wd_mean.copy()\n",
    "    wd = wd.where(~wd.isna(), all_wd_mean).fillna(0.0)\n",
    "    g['wd_mean_local_28'] = wd.astype(float)\n",
    "    return g\n",
    "\n",
    "def sanitize_numeric(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    num_cols = df.select_dtypes(include=['number', 'bool']).columns\n",
    "    df[num_cols] = (df[num_cols].replace([np.inf, -np.inf], np.nan).fillna(0.0))\n",
    "    df[num_cols] = df[num_cols].astype(float)\n",
    "    return df\n",
    "# ------------------------------------------------\n",
    "# 2.6) Global RFM (train 기준으로만 계산 후 전체에 머지)\n",
    "# ------------------------------------------------\n",
    "def generate_rfm_features(\n",
    "    df: pd.DataFrame,\n",
    "    key_col: str = '영업장명_메뉴명',\n",
    "    date_col: str = '영업일자',\n",
    "    qty_col: str = '매출수량',\n",
    "    기준일자: pd.Timestamp | None = None,\n",
    "    fallback_recency_days: int | None = None,\n",
    "    fallback_meanlead_days: int | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Global RFM: df(=train) 전구간에서 key 단위 R/F/M 메타 피처 생성\n",
    "    - recency/meanlead: 미관측 key는 train 기간 길이로 채움\n",
    "    - freq/sum/avgqty: 미관측 key는 0\n",
    "    \"\"\"\n",
    "    assert date_col in df.columns and qty_col in df.columns and key_col in df.columns\n",
    "\n",
    "    # 전역 기본값\n",
    "    train_min = pd.to_datetime(df[date_col].min())\n",
    "    train_max = pd.to_datetime(df[date_col].max())\n",
    "    if 기준일자 is None:\n",
    "        기준일자 = train_max\n",
    "\n",
    "    total_span_days = int((train_max - train_min).days)\n",
    "    if fallback_recency_days is None:\n",
    "        fallback_recency_days = total_span_days\n",
    "    if fallback_meanlead_days is None:\n",
    "        fallback_meanlead_days = total_span_days\n",
    "\n",
    "    keys_all = df[[key_col]].drop_duplicates()\n",
    "    pos = df[df[qty_col] > 0].copy()\n",
    "\n",
    "    if len(pos) > 0:\n",
    "        last_dt   = pos.groupby(key_col)[date_col].max()\n",
    "        recency   = (기준일자 - last_dt).dt.days.rename('rfm_recency')\n",
    "\n",
    "        freq      = pos.groupby(key_col)[date_col].nunique().rename('rfm_freq_days')\n",
    "        summ      = pos.groupby(key_col)[qty_col].sum().rename('rfm_sum_qty')\n",
    "\n",
    "        diffs     = (pos.sort_values([key_col, date_col])\n",
    "                        .assign(_diff=lambda x: x.groupby(key_col)[date_col].diff().dt.days))\n",
    "        meanlead  = diffs.groupby(key_col)['_diff'].mean().rename('rfm_meanlead')\n",
    "\n",
    "        rfm = pd.concat([recency, freq, summ, meanlead], axis=1).reset_index()\n",
    "    else:\n",
    "        rfm = pd.DataFrame(columns=[key_col, 'rfm_recency','rfm_freq_days','rfm_sum_qty','rfm_meanlead'])\n",
    "\n",
    "    rfm = keys_all.merge(rfm, on=key_col, how='left')\n",
    "\n",
    "    rfm['rfm_recency']   = rfm['rfm_recency'].fillna(fallback_recency_days).clip(lower=0)\n",
    "    rfm['rfm_meanlead']  = rfm['rfm_meanlead'].fillna(fallback_meanlead_days).clip(lower=0)\n",
    "    rfm['rfm_freq_days'] = rfm['rfm_freq_days'].fillna(0).clip(lower=0)\n",
    "    rfm['rfm_sum_qty']   = rfm['rfm_sum_qty'].fillna(0.0).clip(lower=0.0)\n",
    "\n",
    "    rfm['rfm_avgqty'] = (rfm['rfm_sum_qty'] / rfm['rfm_freq_days'].replace(0, np.nan))\n",
    "    rfm['rfm_avgqty'] = rfm['rfm_avgqty'].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "\n",
    "    return rfm[[key_col, 'rfm_recency', 'rfm_freq_days', 'rfm_sum_qty',\n",
    "                'rfm_meanlead', 'rfm_avgqty']]\n",
    "\n",
    "# -------------------------\n",
    "# 3) 데이터 로드\n",
    "# -------------------------\n",
    "train = pd.read_csv(TRAIN_PATH, parse_dates=['영업일자'])\n",
    "train['매출수량'] = pd.to_numeric(train['매출수량'], errors='coerce').clip(lower=0)\n",
    "train['영업장명_메뉴명'] = train['영업장명_메뉴명'].astype(str).str.strip()\n",
    "train[['업장명', '메뉴명']] = train['영업장명_메뉴명'].str.split('_', n=1, expand=True)\n",
    "\n",
    "test_input_dfs = []\n",
    "for fname in TEST_GLOB:\n",
    "    fpath = TEST_DIR / fname\n",
    "    df = pd.read_csv(fpath, parse_dates=['영업일자'])\n",
    "    df['test_id'] = os.path.splitext(fname)[0]\n",
    "    df['영업장명_메뉴명'] = df['영업장명_메뉴명'].astype(str).str.strip()\n",
    "    if '매출수량' in df.columns:\n",
    "        df['매출수량'] = pd.to_numeric(df['매출수량'], errors='coerce').clip(lower=0)\n",
    "    df[['업장명', '메뉴명']] = df['영업장명_메뉴명'].str.split('_', n=1, expand=True)\n",
    "    test_input_dfs.append(df)\n",
    "test_input_df = pd.concat(test_input_dfs, ignore_index=True)\n",
    "\n",
    "submission_df = pd.read_csv(SAMPLE_SUB_PATH)\n",
    "sub_long = submission_df.melt(id_vars='영업일자', var_name='영업장명_메뉴명', value_name='예측값')\n",
    "sub_long[['업장명', '메뉴명']] = sub_long['영업장명_메뉴명'].astype(str).str.split('_', n=1, expand=True)\n",
    "sub_long['test_id'] = sub_long['영업일자'].str.extract(r'(TEST_\\d+)')\n",
    "sub_long['offset'] = sub_long['영업일자'].str.extract(r'\\+(\\d+)일').astype(int)\n",
    "\n",
    "test_last_dates = {}\n",
    "for fname in TEST_GLOB:\n",
    "    fpath = TEST_DIR / fname\n",
    "    df = pd.read_csv(fpath, parse_dates=['영업일자'])\n",
    "    test_last_dates[os.path.splitext(fname)[0]] = df['영업일자'].max()\n",
    "\n",
    "sub_long['기준일자'] = sub_long['test_id'].map(test_last_dates)\n",
    "sub_long['영업일자'] = sub_long['기준일자'] + pd.to_timedelta(sub_long['offset'], unit='D')\n",
    "test_pred_df = sub_long[['test_id', '영업일자', '업장명', '메뉴명', '영업장명_메뉴명']].copy()\n",
    "\n",
    "# -------------------------\n",
    "# 4) 피처링 (날씨 + 시계열 + 시간/캘린더 + 메뉴카테고리)\n",
    "# -------------------------\n",
    "# (4.2) wd_mean_local_28\n",
    "if '매출수량' not in test_input_df.columns:\n",
    "    test_input_df['매출수량'] = 0.0\n",
    "train         = train.groupby('영업장명_메뉴명', group_keys=False).apply(_add_wd_mean_local_28)\n",
    "test_input_df = test_input_df.groupby(['test_id','영업장명_메뉴명'], group_keys=False).apply(_add_wd_mean_local_28)\n",
    "\n",
    "# (4.3) 시계열 파생\n",
    "train_ts = add_filtered_ts_features(\n",
    "    train, group_cols='영업장명_메뉴명',\n",
    "    lags=(7,14), roll_mean_windows=(1,3,7,14,28), roll_std_windows=(3,7,14,28),\n",
    "    use_nz_ratio_28=True, use_trend_7_28=False, use_lag7_diff14=False, use_streak_pos=True\n",
    ")\n",
    "input_ts = add_filtered_ts_features(\n",
    "    test_input_df, group_cols=('test_id','영업장명_메뉴명'),\n",
    "    lags=(7,14), roll_mean_windows=(1,3,7,14,28), roll_std_windows=(3,7,14,28),\n",
    "    use_nz_ratio_28=True, use_trend_7_28=False, use_lag7_diff14=False, use_streak_pos=True\n",
    ")\n",
    "\n",
    "KEEP_PREFIX = ('lag_', 'roll_mean_', 'roll_std_', 'nz_ratio_', 'streak_')\n",
    "ts_cols = sorted(set([c for c in train_ts.columns if c.startswith(KEEP_PREFIX)]) |\n",
    "                 set([c for c in input_ts.columns if c.startswith(KEEP_PREFIX)]))\n",
    "\n",
    "train = train.merge(\n",
    "    train_ts[['영업장명_메뉴명','영업일자'] + ts_cols],\n",
    "    on=['영업장명_메뉴명','영업일자'], how='left'\n",
    ")\n",
    "test_input_df = test_input_df.merge(\n",
    "    input_ts[['test_id','영업장명_메뉴명','영업일자'] + ts_cols],\n",
    "    on=['test_id','영업장명_메뉴명','영업일자'], how='left'\n",
    ")\n",
    "\n",
    "# (4.4) 시간/캘린더 + 메뉴카테고리\n",
    "train         = add_time_features(train)\n",
    "test_input_df = add_time_features(test_input_df)\n",
    "test_pred_df  = add_time_features(test_pred_df)\n",
    "\n",
    "train         = add_calendar_context_features(train)\n",
    "test_input_df = add_calendar_context_features(test_input_df)\n",
    "test_pred_df  = add_calendar_context_features(test_pred_df)\n",
    "\n",
    "train         = classify_menu_features(train)\n",
    "test_input_df = classify_menu_features(test_input_df)\n",
    "test_pred_df  = classify_menu_features(test_pred_df)\n",
    "\n",
    "# (4.5) 예측 DF에 최신 시계열/주중효과 전달\n",
    "latest_ts = (\n",
    "    test_input_df.sort_values('영업일자')\n",
    "    .groupby(['test_id','영업장명_메뉴명']).tail(1)[['test_id','영업장명_메뉴명'] + ts_cols]\n",
    ")\n",
    "test_pred_df = test_pred_df.merge(latest_ts, on=['test_id','영업장명_메뉴명'], how='left')\n",
    "\n",
    "# wd_mean_local_28 최신값 전달\n",
    "train = (train.groupby('영업장명_메뉴명', group_keys=False).apply(_add_wd_mean_local_28))\n",
    "test_input_df = (test_input_df.groupby(['test_id','영업장명_메뉴명'], group_keys=False).apply(_add_wd_mean_local_28))\n",
    "latest_wd = (test_input_df.sort_values('영업일자')\n",
    "             .groupby(['test_id','영업장명_메뉴명']).tail(1)[['test_id','영업장명_메뉴명','wd_mean_local_28']])\n",
    "test_pred_df = test_pred_df.merge(latest_wd, on=['test_id','영업장명_메뉴명'], how='left')\n",
    "\n",
    "# 전역 수치 클렌징 1차\n",
    "train        = sanitize_numeric(train)\n",
    "test_input_df= sanitize_numeric(test_input_df)\n",
    "test_pred_df = sanitize_numeric(test_pred_df)\n",
    "\n",
    "# -------------------------\n",
    "# 4.6) Global RFM 계산(오직 train) 후 전 세트에 머지\n",
    "# -------------------------\n",
    "rfm_train = generate_rfm_features(\n",
    "    train[['영업장명_메뉴명','영업일자','매출수량']].copy(),\n",
    "    기준일자=train['영업일자'].max(),\n",
    ").rename(columns={\n",
    "    'rfm_recency':'rfm_recency_train',\n",
    "    'rfm_freq_days':'rfm_freq_train',\n",
    "    'rfm_sum_qty':'rfm_sum_train',\n",
    "    'rfm_meanlead':'rfm_meanlead_train',\n",
    "    'rfm_avgqty':'rfm_avgqty_train'\n",
    "})\n",
    "\n",
    "train         = train.merge(rfm_train, on='영업장명_메뉴명', how='left')\n",
    "test_input_df = test_input_df.merge(rfm_train, on='영업장명_메뉴명', how='left')\n",
    "test_pred_df  = test_pred_df.merge(rfm_train, on='영업장명_메뉴명', how='left')\n",
    "\n",
    "# 결측 방어 + 전역 수치 클렌징 2차\n",
    "for df_ in (train, test_input_df, test_pred_df):\n",
    "    for c in ['rfm_recency_train','rfm_meanlead_train']:\n",
    "        if c in df_.columns: df_[c] = df_[c].fillna(999)\n",
    "    for c in ['rfm_freq_train','rfm_sum_train','rfm_avgqty_train']:\n",
    "        if c in df_.columns: df_[c] = df_[c].fillna(0.0)\n",
    "train        = sanitize_numeric(train)\n",
    "test_input_df= sanitize_numeric(test_input_df)\n",
    "test_pred_df = sanitize_numeric(test_pred_df)\n",
    "\n",
    "# 무결성 체크(선택): RFM은 test_id와 무관해야 함\n",
    "_chk = (test_input_df.groupby('영업장명_메뉴명')[['rfm_recency_train','rfm_sum_train']]\n",
    "        .nunique())\n",
    "if (_chk > 1).any().any():\n",
    "    raise RuntimeError(\"Global RFM anomaly: _train 피처가 test_id에 따라 달라졌습니다.\")\n",
    "\n",
    "# -------------------------\n",
    "# 5) 입력 피처 선택 + 스케일링\n",
    "# -------------------------\n",
    "TARGET = '매출수량'\n",
    "base_drop_cols = ['영업일자', '영업장명_메뉴명', '업장명', '메뉴명']  # 원천 문자 컬럼 제거\n",
    "\n",
    "# 과거 메타 잔재는 안전 드롭(없어도 errors='ignore')\n",
    "DROP_FEATS = [\n",
    "    \"venue_group_ski\",\"venue_group_hwadam\",\"venue_group_lodging\",\"venue_group_other\",\n",
    "    \"roll_mean_14\",\n",
    "    \"roll_std_14\",\n",
    "    'rfm_sum_train',\n",
    "    'roll_mean_7',\n",
    "    '평균판매금액', \n",
    "    'ski_visitors', \n",
    "    'hwadam_visitors', \n",
    "    'cooking_ratio',\n",
    "    \"최고기온(°C)\",\"일강수량(mm)\"\n",
    "]\n",
    "\n",
    "X_train_tmp = (train.drop(columns=[TARGET] + base_drop_cols + DROP_FEATS, errors='ignore')\n",
    "                    .select_dtypes(include=['number','bool']))\n",
    "expected_features = X_train_tmp.columns.tolist()\n",
    "\n",
    "# 시계열 핵심만 Robust Scaling (날씨/캘린더/RFM은 비스케일)\n",
    "MASTER_SCALER_ORDER = [\n",
    "    'lag_7','lag_14',\n",
    "    'roll_mean_1','roll_mean_3','roll_mean_28',\n",
    "    'roll_std_3','roll_std_7','roll_std_28',\n",
    "    'streak_pos', \n",
    "]\n",
    "scaler_cols = [c for c in MASTER_SCALER_ORDER if c in expected_features]\n",
    "\n",
    "if scaler_cols:\n",
    "    scaler = RobustScaler().fit(train[scaler_cols])\n",
    "    train.loc[:, scaler_cols] = scaler.transform(train[scaler_cols])\n",
    "else:\n",
    "    scaler = None\n",
    "dyn_keep = set(scaler_cols)\n",
    "\n",
    "print(f\"[INFO] 최종 피처 개수: {len(expected_features)}\")\n",
    "print(expected_features)\n",
    "\n",
    "\n",
    "# 전처리/모델 (기본)\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler, TargetEncoder\n",
    "from sklearn.linear_model import (\n",
    "    LinearRegression, Ridge, Lasso, ElasticNet,\n",
    "    HuberRegressor, SGDRegressor, PoissonRegressor, TweedieRegressor\n",
    ")\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor, ExtraTreesRegressor,\n",
    "    GradientBoostingRegressor, HistGradientBoostingRegressor,\n",
    "    AdaBoostRegressor, BaggingRegressor\n",
    ")\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# -------------------------\n",
    "# 6) 모델 ZOO\n",
    "# -------------------------\n",
    "def make_model_zoo(random_state=42):\n",
    "    rs = random_state\n",
    "    zoo = {}\n",
    "    zoo[\"Elastic\"]      = (\"ElasticNet\",    lambda: ElasticNet(alpha=0.001, l1_ratio=0.5, random_state=rs, max_iter=5000))\n",
    "\n",
    "    try:\n",
    "        from lightgbm import LGBMRegressor\n",
    "        base_lgbm = dict(\n",
    "            n_estimators=1000, learning_rate=0.05,\n",
    "            num_leaves=63, subsample=0.8, colsample_bytree=0.8,\n",
    "            n_jobs=-1, random_state=rs\n",
    "        )\n",
    "        zoo[\"LGBM_Tweedie\"] = (\"LGBM-Tweedie\",    lambda: LGBMRegressor(objective=\"tweedie\",\n",
    "                                                                        tweedie_variance_power=1.2, **base_lgbm))\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        from xgboost import XGBRegressor\n",
    "        base_xgb = dict(\n",
    "            n_estimators=900, max_depth=8, learning_rate=0.05,\n",
    "            subsample=0.8, colsample_bytree=0.8,\n",
    "            tree_method=\"hist\", n_jobs=-1, random_state=rs\n",
    "        )\n",
    "        zoo[\"XGB_Poisson\"]  = (\"XGB-Poisson\",     lambda: XGBRegressor(objective=\"count:poisson\", **base_xgb))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return zoo\n",
    "\n",
    "# -------------------------\n",
    "# 7) AR 동적 피처 & 학습/예측 유틸\n",
    "# -------------------------\n",
    "def _model_feature_names(model, fallback_cols):\n",
    "    if hasattr(model, \"_feature_list\") and getattr(model, \"_feature_list\"):\n",
    "        return list(model._feature_list)\n",
    "    if hasattr(model, 'feature_name_') and getattr(model, 'feature_name_', None):\n",
    "        return list(model.feature_name_)\n",
    "    try:\n",
    "        return list(model.booster_.feature_name())\n",
    "    except Exception:\n",
    "        return list(fallback_cols)\n",
    "\n",
    "def train_global_models(train_df, expected_features, model_name_list, random_state=42, target_col='매출수량'):\n",
    "    zoo = make_model_zoo(random_state=random_state)\n",
    "    from collections import OrderedDict\n",
    "    trained = OrderedDict()\n",
    "    X = (train_df.reindex(columns=expected_features, fill_value=0.0)\n",
    "                 .replace([np.inf, -np.inf], np.nan)\n",
    "                 .fillna(0.0)\n",
    "                 .astype(float))\n",
    "    y = train_df[target_col].astype(float).values\n",
    "    for name in model_name_list:\n",
    "        if name not in zoo:\n",
    "            print(f\"[SKIP] {name}: not available\")\n",
    "            continue\n",
    "        try:\n",
    "            mdl = zoo[name][1]()\n",
    "            mdl.fit(X, y)\n",
    "            setattr(mdl, \"_feature_list\", expected_features[:])\n",
    "            trained[name] = mdl\n",
    "        except Exception as e:\n",
    "            print(f\"[SKIP] {name}: {e}\")\n",
    "    return trained\n",
    "\n",
    "def _compute_step_ts_features(window_sales: pd.Series, need_cols: set[str]) -> dict:\n",
    "    f = {}\n",
    "    if window_sales is None or len(window_sales) == 0 or not need_cols:\n",
    "        return f\n",
    "    s = pd.to_numeric(window_sales, errors='coerce').fillna(0.0).astype(float)\n",
    "    for k in (7, 14):\n",
    "        key = f\"lag_{k}\"\n",
    "        if key in need_cols:\n",
    "            f[key] = float(s.iloc[-k]) if len(s) >= k else 0.0\n",
    "    def _mean_last(w):\n",
    "        tail = s.tail(w)\n",
    "        return float(tail.mean()) if len(tail) > 0 else 0.0\n",
    "    for w in (1, 3, 7, 14, 28):\n",
    "        key = f\"roll_mean_{w}\"\n",
    "        if key in need_cols: f[key] = _mean_last(w)\n",
    "    def _std_last(w):\n",
    "        tail = s.tail(w)\n",
    "        return float(tail.std(ddof=1)) if len(tail) > 1 else 0.0\n",
    "    for w in (3, 7, 14, 28):\n",
    "        key = f\"roll_std_{w}\"\n",
    "        if key in need_cols: f[key] = _std_last(w)\n",
    "    if \"nz_ratio_28\" in need_cols:\n",
    "        tail = s.tail(28)\n",
    "        f[\"nz_ratio_28\"] = float((tail > 0).mean()) if len(tail) > 0 else 0.0\n",
    "    if \"streak_pos\" in need_cols:\n",
    "        cnt = 0\n",
    "        for v in reversed(s.values):\n",
    "            if v > 0: cnt += 1\n",
    "            else: break\n",
    "        f[\"streak_pos\"] = float(cnt)\n",
    "    return f\n",
    "\n",
    "def auto_regressive_predict_global_ensemble(\n",
    "    models_dict, weights_dict=None, blend=\"weighted_mean\",\n",
    "    test_input_df=None, test_pred_df=None,\n",
    "    drop_cols=None, expected_features=None,\n",
    "    scaler_cols=None, scaler=None,\n",
    "    window_size=28, dyn_keep=None, target_col='매출수량'\n",
    "):\n",
    "    assert blend in (\"weighted_mean\", \"mean\", \"median\")\n",
    "    model_names = list(models_dict.keys())\n",
    "    models = [models_dict[n] for n in model_names]\n",
    "    feats_list = [_model_feature_names(m, expected_features) for m in models]\n",
    "    if blend == \"weighted_mean\":\n",
    "        if (weights_dict is None) or (len(weights_dict)==0):\n",
    "            w = np.ones(len(models), dtype=float)\n",
    "        else:\n",
    "            w = np.array([weights_dict.get(n, 0.0) for n in model_names], dtype=float)\n",
    "        w = np.clip(w, 0, None)\n",
    "        w = w / (w.sum() if w.sum() > 0 else 1.0)\n",
    "    else:\n",
    "        w = np.ones(len(models), dtype=float) / max(1, len(models))\n",
    "    dyn_keep = set(dyn_keep) if dyn_keep is not None else set(scaler_cols or [])\n",
    "    dyn_keep &= set(expected_features or [])\n",
    "    preds_all, nan_cnt, nan_examples = [], 0, []\n",
    "    for (test_id, key), pred_group in test_pred_df.groupby(['test_id','영업장명_메뉴명']):\n",
    "        cond = (test_input_df['test_id']==test_id) & (test_input_df['영업장명_메뉴명']==key)\n",
    "        window = test_input_df[cond].sort_values('영업일자').copy()\n",
    "        if '매출수량' not in window.columns:\n",
    "            window['매출수량'] = 0.0\n",
    "        rows = []\n",
    "        for _, row in pred_group.sort_values('영업일자').iterrows():\n",
    "            if dyn_keep:\n",
    "                step_vals = _compute_step_ts_features(window[target_col], dyn_keep)\n",
    "                if step_vals:\n",
    "                    row.loc[list(step_vals.keys())] = list(step_vals.values())\n",
    "            if scaler_cols and scaler is not None:\n",
    "                vec = np.array([row.get(c, 0.0) for c in scaler_cols], dtype=float).reshape(1, -1)\n",
    "                vec = np.nan_to_num(vec, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                vec_scaled = scaler.transform(vec)[0]\n",
    "                for c, v in zip(scaler_cols, vec_scaled):\n",
    "                    row[c] = v\n",
    "            m_preds = []\n",
    "            for mdl, feats in zip(models, feats_list):\n",
    "                xi = (row.drop(labels=(drop_cols or []) + [target_col], errors='ignore')\n",
    "                        .reindex(feats, fill_value=0.0)\n",
    "                        .astype(float)\n",
    "                        .values.reshape(1,-1))\n",
    "                try:\n",
    "                    m_preds.append(float(mdl.predict(xi)[0]))\n",
    "                except Exception:\n",
    "                    m_preds.append(np.nan)\n",
    "            m_preds = np.array(m_preds, dtype=float)\n",
    "            if blend == \"median\":\n",
    "                pred_raw = np.nanmedian(m_preds)\n",
    "            elif blend == \"mean\":\n",
    "                pred_raw = np.nanmean(m_preds)\n",
    "            else:\n",
    "                mask = np.isfinite(m_preds)\n",
    "                if mask.any():\n",
    "                    ww = w.copy(); ww[~mask] = 0.0\n",
    "                    s = ww.sum()\n",
    "                    pred_raw = float(np.dot(ww/s if s>0 else ww, np.nan_to_num(m_preds, nan=0.0))) if s>0 else float(np.nanmean(m_preds))\n",
    "                else:\n",
    "                    nan_cnt += 1\n",
    "                    if len(nan_examples) < 5:\n",
    "                        nan_examples.append({'test_id': test_id, '영업장명_메뉴명': key, '영업일자': row['영업일자']})\n",
    "                    pred_raw = 0.0\n",
    "            pred = np.clip(np.nan_to_num(pred_raw, nan=0.0), 0, None)\n",
    "            rows.append({'test_id': test_id, '영업일자': row['영업일자'], '영업장명_메뉴명': key, '예측값': pred})\n",
    "            add_row = row.copy(); add_row[target_col] = pred\n",
    "            window = pd.concat([window, add_row.to_frame().T], ignore_index=True).sort_values('영업일자').iloc[-window_size:]\n",
    "        if rows:\n",
    "            preds_all.append(pd.DataFrame(rows))\n",
    "    if nan_cnt>0 and nan_examples:\n",
    "        print(f\"[진단] (앙상블) raw NaN 예측 건수: {nan_cnt}\")\n",
    "        print(\"[예시] NaN 발생 샘플:\", *nan_examples[:5], sep=\"\\n\")\n",
    "    return pd.concat(preds_all, ignore_index=True) if preds_all else \\\n",
    "           pd.DataFrame(columns=['test_id','영업일자','영업장명_메뉴명','예측값'])\n",
    "\n",
    "# -------------------------\n",
    "# 8) (추가) AR-CV 수집 + SMAPE 출력\n",
    "# -------------------------\n",
    "def _safe_cutoffs(train_df: pd.DataFrame, n_folds: int, horizon: int, date_col: str = \"영업일자\"):\n",
    "    days = np.array(sorted(pd.to_datetime(train_df[date_col].unique())))\n",
    "    need = n_folds*horizon + 1\n",
    "    if len(days) < need:\n",
    "        step = max(1, len(days)//(n_folds+1))\n",
    "        base = len(days) - horizon - 1\n",
    "        idxs = list(range(base - (n_folds-1)*step, base+1, step))\n",
    "        idxs = [i for i in idxs if 0 <= i < len(days)-1]\n",
    "        return [days[i] for i in idxs]\n",
    "    return list(days[-need:-1:horizon])\n",
    "\n",
    "def run_time_series_cv_ar_collect(\n",
    "    train_df,\n",
    "    features,\n",
    "    target_col=\"매출수량\",\n",
    "    n_folds=3,\n",
    "    horizon=7,\n",
    "    dyn_keep=None,\n",
    "    scaler=None,\n",
    "    scaler_cols=None,\n",
    "    drop_cols=None,\n",
    "    window_size=28,\n",
    "    model_list=None\n",
    "):\n",
    "    train_df = train_df.sort_values(\"영업일자\").copy()\n",
    "    cutoffs = _safe_cutoffs(train_df, n_folds=n_folds, horizon=horizon, date_col=\"영업일자\")\n",
    "    if len(cutoffs) == 0:\n",
    "        print(\"[경고] 유효한 CV 컷오프가 없습니다.\")\n",
    "        empty_oof = train_df[['업장명','영업일자', target_col]].copy()\n",
    "        return empty_oof, pd.DataFrame(columns=[\"RMSE\",\"MAE\",\"SMAPE\"])\n",
    "\n",
    "    zoo_all = make_model_zoo()\n",
    "    model_list = list(zoo_all.keys()) if model_list is None else model_list\n",
    "    zoo = {k: zoo_all[k] for k in model_list if k in zoo_all}\n",
    "\n",
    "    oof = train_df[['업장명','영업일자', target_col]].copy()\n",
    "    for name in zoo.keys():\n",
    "        oof[f'pred__{name}'] = np.nan\n",
    "\n",
    "    model_scores = []\n",
    "\n",
    "    for cutoff in cutoffs:\n",
    "        fold_train = train_df[train_df['영업일자'] <= cutoff].copy()\n",
    "        fold_valid = train_df[(train_df['영업일자'] > cutoff) &\n",
    "                              (train_df['영업일자'] <= cutoff + pd.Timedelta(days=horizon))].copy()\n",
    "        if fold_train.empty or fold_valid.empty:\n",
    "            continue\n",
    "\n",
    "        base_windows = {}\n",
    "        for key, g in fold_train.groupby('영업장명_메뉴명'):\n",
    "            s = pd.to_numeric(g.sort_values('영업일자')[target_col], errors='coerce').fillna(0.0)\n",
    "            base_windows[key] = s\n",
    "\n",
    "        X_train = (fold_train[features]\n",
    "                   .replace([np.inf, -np.inf], np.nan)\n",
    "                   .fillna(0.0)\n",
    "                   .astype(float))\n",
    "        y_train = fold_train[target_col].astype(float)\n",
    "\n",
    "        for name,(_,builder) in zoo.items():\n",
    "            try:\n",
    "                mdl = builder()\n",
    "                mdl.fit(X_train, y_train)\n",
    "                preds_idx, preds_val = [], []\n",
    "                window_by_key = {k: v.copy() for k, v in base_windows.items()}\n",
    "                for day in sorted(fold_valid['영업일자'].unique()):\n",
    "                    day_rows = fold_valid[fold_valid['영업일자']==day].copy()\n",
    "                    for idx, row in day_rows.iterrows():\n",
    "                        key = row['영업장명_메뉴명']\n",
    "                        s = window_by_key.get(key, pd.Series(dtype=float))\n",
    "                        if dyn_keep:\n",
    "                            step_vals = _compute_step_ts_features(s, set(dyn_keep))\n",
    "                            if step_vals:\n",
    "                                for k,v in step_vals.items(): row[k] = v\n",
    "                        if scaler is not None and scaler_cols:\n",
    "                            vec = np.array([row.get(c, 0.0) for c in scaler_cols], dtype=float).reshape(1, -1)\n",
    "                            vec = np.nan_to_num(vec, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                            scaled = scaler.transform(vec)[0]\n",
    "                            for c,v in zip(scaler_cols, scaled):\n",
    "                                row[c] = float(v)\n",
    "                        xi = (row.drop(labels=(drop_cols or [])+[target_col], errors='ignore')\n",
    "                                .reindex(features, fill_value=0.0)\n",
    "                                .astype(float)\n",
    "                                .values.reshape(1,-1))\n",
    "                        pred_raw = float(mdl.predict(xi)[0])\n",
    "                        # 점수는 제출 정책과 동일하게 1min 클리핑 고려 시:\n",
    "                        pred_score = 1.0 if (not np.isfinite(pred_raw) or pred_raw <= 1.0) else pred_raw\n",
    "                        preds_idx.append(idx); preds_val.append(pred_score)\n",
    "                        pred_for_window = 0.0 if (not np.isfinite(pred_raw) or pred_raw < 0.0) else pred_raw\n",
    "                        s_new = pd.concat([s, pd.Series([pred_for_window])], ignore_index=True)\n",
    "                        window_by_key[key] = s_new.iloc[-window_size:]\n",
    "                if preds_idx:\n",
    "                    oof.loc[preds_idx, f'pred__{name}'] = preds_val\n",
    "                    tmp = fold_valid[[target_col,'업장명']].copy()\n",
    "                    tmp['예측값'] = pd.Series(preds_val, index=preds_idx)\n",
    "                    tmp = tmp.dropna()\n",
    "                    if len(tmp) > 0:\n",
    "                        rmse = mean_squared_error(tmp[target_col], tmp['예측값'], squared=False)\n",
    "                        mae  = mean_absolute_error(tmp[target_col], tmp['예측값'])\n",
    "                        s    = smape_group_macro(tmp)\n",
    "                        model_scores.append((name, rmse, mae, s))\n",
    "            except Exception as e:\n",
    "                print(f\"[SKIP fold] {name}: {e}\")\n",
    "\n",
    "    res_df = pd.DataFrame(model_scores, columns=[\"Model\",\"RMSE\",\"MAE\",\"SMAPE\"])\n",
    "    model_cv = res_df.groupby(\"Model\", as_index=True).mean().sort_values(\"SMAPE\") if len(res_df) else \\\n",
    "               pd.DataFrame(columns=[\"RMSE\",\"MAE\",\"SMAPE\"])\n",
    "    print(\"\\n[단일 모델 AR-CV 결과] (SMAPE 오름차순)\")\n",
    "    display(model_cv)\n",
    "    return oof, model_cv\n",
    "\n",
    "def evaluate_ensembles_on_oof(oof_df, weights_dict: dict, target_col='매출수량'):\n",
    "    cols, weights = [], []\n",
    "    for m, w in weights_dict.items():\n",
    "        col = f'pred__{m}'\n",
    "        if col in oof_df.columns and w>0:\n",
    "            cols.append(col); weights.append(float(w))\n",
    "    if not cols:\n",
    "        print(\"[앙상블 CV] 사용할 예측 열이 없습니다.\")\n",
    "        return np.nan\n",
    "    W = np.array(weights, dtype=float)\n",
    "    W = W / W.sum() if W.sum()>0 else np.ones(len(W))/len(W)\n",
    "    P = oof_df[cols].values\n",
    "    mask = ~np.isnan(P)\n",
    "    denom = (mask * W).sum(axis=1, keepdims=True)\n",
    "    denom[denom==0] = np.nan\n",
    "    blend = (np.nan_to_num(P) * W).sum(axis=1, keepdims=True) / denom\n",
    "    blend = blend.ravel()\n",
    "    tmp = pd.DataFrame({'업장명': oof_df['업장명'], target_col: oof_df[target_col], '예측값': blend}).dropna()\n",
    "    s = smape_group_macro(tmp)\n",
    "    print(f\"[앙상블 CV SMAPE] {s:.6f}  (가중치: {weights_dict})\")\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b3e6ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003771 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3344\n",
      "[LightGBM] [Info] Number of data points in the train set: 88844, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score 2.482751\n",
      "[OK] 저장 완료: submission_noweather.csv  shape=(70, 167)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 10) 고정 앙상블: 전체 학습 → 7일 AR 예측 → 제출 파일 생성\n",
    "# ============================================================\n",
    "\n",
    "# 1) 사용할 모델과 가중치 (설치된 모델만 자동 사용)\n",
    "fixed_models  = [\"XGB_Poisson\",\"LGBM_Tweedie\",\"Elastic\"]\n",
    "fixed_weights = {\"XGB_Poisson\":0.5, \"LGBM_Tweedie\":0.45, \"Elastic\":0.05}\n",
    "\n",
    "# 2) 전체 train으로 모델 학습\n",
    "trained = train_global_models(\n",
    "    train_df=train,\n",
    "    expected_features=expected_features,\n",
    "    model_name_list=fixed_models,\n",
    "    random_state=42,\n",
    "    target_col=TARGET\n",
    ")\n",
    "\n",
    "if len(trained) == 0:\n",
    "    raise RuntimeError(\"사용 가능한 모델이 없습니다. XGBoost/LightGBM 설치 여부를 확인하세요.\")\n",
    "\n",
    "# 3) 7일 자동회귀 예측 (고정 앙상블)\n",
    "preds_long = auto_regressive_predict_global_ensemble(\n",
    "    models_dict=trained,\n",
    "    weights_dict=fixed_weights,         # 설치/학습된 모델만 내부에서 자동 정규화\n",
    "    blend=\"weighted_mean\",\n",
    "    test_input_df=test_input_df,        # 히스토리\n",
    "    test_pred_df=test_pred_df,          # 예측할 미래 날짜 표\n",
    "    drop_cols=base_drop_cols,\n",
    "    expected_features=expected_features,\n",
    "    scaler_cols=scaler_cols,\n",
    "    scaler=scaler,\n",
    "    window_size=28,\n",
    "    dyn_keep=dyn_keep,\n",
    "    target_col=TARGET\n",
    ")\n",
    "\n",
    "# 4) 제출 포맷 복원 (sample_submission과 동일 인덱스/컬럼)\n",
    "#    - sub_long에는 이미 test_id, offset, (실제)영업일자가 들어 있음\n",
    "sub_for_join = sub_long[['test_id','offset','영업장명_메뉴명','영업일자']].copy()\n",
    "sub_for_join['제출키'] = sub_for_join['test_id'] + '+' + sub_for_join['offset'].astype(int).astype(str) + '일'\n",
    "\n",
    "merge_df = sub_for_join.merge(\n",
    "    preds_long[['test_id','영업장명_메뉴명','영업일자','예측값']],\n",
    "    on=['test_id','영업장명_메뉴명','영업일자'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 결측 방어 및 음수 방지\n",
    "merge_df['예측값'] = pd.to_numeric(merge_df['예측값'], errors='coerce').fillna(0.0)\n",
    "merge_df['예측값'] = merge_df['예측값'].clip(lower=0)\n",
    "\n",
    "# 5) Wide 피벗 → sample_submission 스켈레톤에 채워 넣기\n",
    "pred_wide = merge_df.pivot(index='제출키', columns='영업장명_메뉴명', values='예측값')\n",
    "\n",
    "# sample_submission의 행/열 정렬을 그대로 유지\n",
    "out_df = submission_df.set_index('영업일자').copy()\n",
    "# 교집합 위치에 값 반영\n",
    "out_df.update(pred_wide)\n",
    "\n",
    "# # 6) 저장 (UTF-8-sig 요구사항)\n",
    "SUBMIT_PATH = \"submission_noweather.csv\"\n",
    "out_df.reset_index().to_csv(SUBMIT_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"[OK] 저장 완료: {SUBMIT_PATH}  shape={out_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d580a9",
   "metadata": {},
   "source": [
    "# 두 모델 결과 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cdcf38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_submission.csv 파일을 찾을 수 없어 컬럼 순서 정렬을 건너뜁니다.\n",
      "두 개의 제출 파일의 예측값을 단순 평균한 새로운 제출 파일(2_high_average.csv)이 생성되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataframes for temp_1 and temp_2\n",
    "try:\n",
    "    temp_1 = pd.read_csv('submission_noweather.csv')\n",
    "    temp_2 = pd.read_csv('../data/submission.csv')\n",
    "    \n",
    "    # Melt each dataframe to a long format\n",
    "    melted_1 = temp_1.melt(id_vars='영업일자', var_name='영업장명_메뉴명', value_name='pred_1')\n",
    "    melted_2 = temp_2.melt(id_vars='영업일자', var_name='영업장명_메뉴명', value_name='pred_2')\n",
    "    \n",
    "    # Merge the two dataframes\n",
    "    merged_preds = pd.merge(melted_1, melted_2, on=['영업일자', '영업장명_메뉴명'], how='inner')\n",
    "    \n",
    "    # Calculate the simple average of the two predictions\n",
    "    merged_preds['final_pred'] = (merged_preds['pred_1'] + merged_preds['pred_2']) / 2\n",
    "    \n",
    "    # Clip negative values to 1\n",
    "    merged_preds['final_pred'] = merged_preds['final_pred'].clip(lower=1)\n",
    "    \n",
    "    # Pivot the dataframe back to the final submission format\n",
    "    final_submission = merged_preds.pivot(\n",
    "        index='영업일자',\n",
    "        columns='영업장명_메뉴명',\n",
    "        values='final_pred'\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Reindex columns to match the sample submission if available\n",
    "    try:\n",
    "        sample_sub = pd.read_csv('data/sample_submission.csv')\n",
    "        sample_cols = sample_sub.columns.tolist()\n",
    "        final_submission = final_submission.reindex(columns=sample_cols, fill_value=1)\n",
    "    except FileNotFoundError:\n",
    "        print(\"sample_submission.csv 파일을 찾을 수 없어 컬럼 순서 정렬을 건너뜁니다.\")\n",
    "    \n",
    "    # Save the final dataframe to a CSV file\n",
    "    final_submission.to_csv('2_deep_average_.csv', index=False)\n",
    "    \n",
    "    print(\"두 개의 제출 파일의 예측값을 단순 평균한 새로운 제출 파일(2_high_average.csv)이 생성되었습니다.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. 하나 이상의 입력 파일을 찾을 수 없습니다. 파일 경로를 확인해 주세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da5bb60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
